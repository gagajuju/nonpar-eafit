# Density estimation {#dens}

A random variable $X$ is completely characterized by its cdf. Hence, an estimation of the cdf yields as a side-product estimates for different characteristics of $X$ by plugging-in $F_n$ in the $F$. For example, the mean $\mu=\mathbb{E}[X]=\int x \mathrm{d}F(x)$ can be estimated by $\int x \mathrm{d}F_n(x)=\frac{1}{n}\sum_{i=1}^n X_i=\bar X$. Despite its usefulness, cdfs are hard to visualize and interpret.

**Densities**, on the other hand, are easy to visualize and interpret, making them **ideal tools for data exploration**. They provide immediate graphical information about the most likely areas, modes, and spread of $X$. A *continuous* random variable is also completely characterized by its pdf $f=F'$. Density estimation does not follow trivially from the ecdf $F_n$, since this is not differentiable (not even continuous), hence the need of the specific procedures we will see in this chapter.

## Histograms {#dens-hist}

### Histogram {#dens-histo}

The simplest method to estimate a density $f$ form an iid sample $X_1,\ldots,X_n$ is the *histogram*. From an analytical point of view, the idea is to aggregate the data in intervals of the form $[x_0,x_0+h)$ and then use their relative frequency to approximate the density at $x\in[x_0,x_0+h)$, $f(x)$, by the estimate of
\begin{align*}
f(x_0)&=F'(x_0)\\
&=\lim_{h\to0^+}\frac{F(x_0+h)-F(x_0)}{h}\\
&=\lim_{h\to0^+}\frac{\mathbb{P}[x_0<X< x_0+h]}{h}.
\end{align*}

More precisely, given an origin $t_0$ and a *bandwidth* $h>0$, the histogram builds a piecewise constant function in the intervals $\{B_k:=[t_{k},t_{k+1}):t_k=t_0+hk,k\in\mathbb{Z}\}$ by counting the number of sample points inside each of them. These constant-length intervals are also denoted *bins*. The fact that they are of constant length $h$ is important, since it allows to standardize by $h$ in order to have relative frequencies in the bins. The histogram at a point $x$ is defined as
\begin{align}
\hat f_H(x;t_0,h):=\frac{1}{nh}\sum_{i=1}^n1_{\{X_i\in B_k:x\in B_k\}}. (\#eq:hist)
\end{align}
Equivalently, if we denote the number of points in $B_k$ as $v_k$, then the histogram is $\hat f_H(x;t_0,h)=\frac{v_k}{nh}$ if $x\in B_k$ for a $k\in\mathbb{Z}$.

The analysis of $\hat f_H(x;t_0,h)$ as a random variable is simple, once it is recognized that the bin counts $v_k$ are distributed as $\mathrm{B}(n,p_k)$, with $p_k=\mathbb{P}[X\in B_k]=\int_{B_k} f(t)\mathrm{d}t$^[Note that it is key that the $\{B_k\}$ are *deterministic* (and not sample-dependent) for this result to hold.]. If $f$ is continuous, then by the mean value theorem, $p_k=hf(\xi_{k,h})$ for a $\xi_{k,h}\in(t_k,t_{k+1})$. Therefore:
\begin{align*}
\mathbb{E}[\hat f_H(x;t_0,h)]&=\frac{np_k}{nh}=f(\xi_{k,h}),\\
\mathbb{V}\mathrm{ar}[\hat f_H(x;t_0,h)]&=\frac{np_k(1-p_k)}{n^2h^2}=\frac{f(\xi_{k,h})(1-hf(\xi_{k,h}))}{nh}.
\end{align*}
The above results show interesting insights: 

1. If $h\to0$, then $\xi_{h,k}\to x$, resulting in $f(\xi_{k,h})\to f(x)$, and thus \@ref(eq:hist) becoming an *asymptotically* unbiased estimator of $f(x)$.
2. But if $h\to0$, the variance increases. For decreasing the variance, $nh\to\infty$ is required. 
3. The variance is directly dependent on $f(x)(1-hf(x))\approx f(x)$, hence the more variability at regions with higher density. 

A more detailed analysis of the histogram can be seen in Section 3.2.2 of @Scott2015. We skip it here since we the detailed asymptotic analysis for the more general kernel density estimator is given in Section \@ref(dens-kde).

Implementation of histograms is very simple in `R`. As an example, we consider the old-but-gold dataset `faithful`. This dataset contains the duration of the eruption and the waiting time between eruptions for the Old Faithful geyser in Yellowstone National Park (USA).

```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# The faithful dataset is included in R
head(faithful)

# Duration of eruption
faithE <- faithful$eruptions

# Default histogram: automatically chosen bins and absolute frequencies!
histo <- hist(faithE)

# List that contains several objects
str(histo)

# With relative frequencies
hist(faithE, probability = TRUE)

# Choosing the breaks
# t0 = min(faithE), h = 0.25
Bk <- seq(min(faithE), max(faithE), by = 0.25)
hist(faithE, probability = TRUE, breaks = Bk)
rug(faithE) # Plotting the sample
```

The shape of the histogram depends on:

- $t_0$, since the separation between bins happens at $t_0k$, $k\in\mathbb{Z}$;
- $h$, which controls the bin size and the effective number of bins for aggregating the sample. 

We focus first on exploring the dependence on $t_0$, as it serves for motivating the next density estimator.
```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# Uniform sample
set.seed(1234567)
u <- runif(n = 100)

# t0 = 0, h = 0.2
Bk1 <- seq(0, 1, by = 0.2)

# t0 = -0.1, h = 0.2
Bk2 <- seq(-0.1, 1.1, by = 0.2)

# Comparison
par(mfrow = c(1, 2))
hist(u, probability = TRUE, breaks = Bk1, ylim = c(0, 1.5),
     main = "t0 = 0, h = 0.2")
rug(u)
abline(h = 1, col = 2)
hist(u, probability = TRUE, breaks = Bk2, ylim = c(0, 1.5),
     main = "t0 = -0.1, h = 0.2")
rug(u)
abline(h = 1, col = 2)
```

High dependence on $t_0$ also happens when estimating densities that are not compactly supported. The next snippet of code points towards it.
```{r, echo = TRUE, eval = FALSE, collapse = TRUE, cache = TRUE}
# Sample 100 points from a N(0, 1) and 50 from a N(3, 0.25)
set.seed(1234567)
samp <- c(rnorm(n = 100, mean = 0, sd = 1), 
          rnorm(n = 50, mean = 3.25, sd = sqrt(0.5)))

# min and max for choosing Bk1 and Bk2
range(samp)

# Comparison
Bk1 <- seq(-2.5, 5, by = 0.5)
Bk2 <- seq(-2.25, 5.25, by = 0.5)
par(mfrow = c(1, 2))
hist(samp, probability = TRUE, breaks = Bk1, ylim = c(0, 0.5), 
     main = "t0 = -2.5, h = 0.5")
rug(samp)
hist(samp, probability = TRUE, breaks = Bk2, ylim = c(0, 0.5),
     main = "t0 = -2.25, h = 0.5")
rug(samp)
```

### Moving histogram {#dens-movhist}

An alternative to avoid the dependence on $t_0$ is the *moving histogram* or *naive density estimator*. The idea is to aggregate the sample $X_1,\ldots,X_n$ in intervals of the form $(x-h, x+h)$ and then use its relative frequency in $(x-h,x+h)$ to approximate the density at $x$: 
\begin{align*}
f(x)&=F'(x)\\
&=\lim_{h\to0^+}\frac{F(x+h)-F(x-h)}{2h}\\
&=\lim_{h\to0^+}\frac{\mathbb{P}[x-h<X<x+h]}{2h}.
\end{align*}
Recall the differences with the histogram: the intervals depend on the evaluation point $x$ and are centered around it. That allows to directly estimate $f(x)$ (without the proxy $f(x_0)$) by an estimate of the symmetric derivative. 

More precisely, given a bandwidth $h>0$, the naive density estimator builds a piecewise constant function by considering the relative frequency of$X_1,\ldots,X_n$ inside $(x-h,x+h)$:
\begin{align}
\hat f_N(x;h):=\frac{1}{2nh}\sum_{i=1}^n1_{\{x-h<X_i<x+h\}}. (\#eq:movhist)
\end{align}
The function has $2n$ discontinuities that are located at $X_i\pm h$. 

Similarly to the histogram, the analysis of $\hat f_N(x;h)$ as a random variable follows from realizing that $\sum_{i=1}^n1_{\{x-h<X_i<x+h\}}\sim \mathrm{B}(n,p_{x,h})$, $p_{x,h}=\mathbb{P}[x-h<X<x+h]=F(x+h)-F(x-h)$. Then:
\begin{align*}
\mathbb{E}[\hat f_N(x;h)]=&\,\frac{F(x+h)-F(x-h)}{2h},\\
\mathbb{V}\mathrm{ar}[\hat f_N(x;h)]=&\,\frac{F(x+h)-F(x-h)}{4nh^2}\\
&-\frac{(F(x+h)-F(x-h))^2}{4nh^2}.
\end{align*}
These two results provide interesting insights on the effect of $h$:

1. If $h\to0$, then $\mathbb{E}[\hat f_N(x;h)]\to f(x)$ and \@ref(eq:movhist) is an asymptotically unbiased estimator of $f(x)$. But also $\mathbb{V}\mathrm{ar}[\hat f_N(x;h)]\approx \frac{f(x)}{2nh}-\frac{f(x)^2}{n}\to\infty$.
2. If $h\to\infty$, then $\mathbb{E}[\hat f_N(x;h)]\to \infty$ and $\mathbb{V}\mathrm{ar}[\hat f_N(x;h)]\to0$.
3. The variance shrinks to zero if $nh\to\infty$ (or, in other words, if $h^{-1}=o(n)$). So both the bias and the variance can be shrink to zero reduced if $n\to\infty$, $h\to0$ and $nh\to\infty$.
4. The variance is (almost) proportional to $f(x)$.

The animation in Figure \@ref(fig:movinghist) illustrates the previous points and gives insight on how the performance of \@ref(eq:movhist) varies smoothly with $h$.

```{r, movinghist, echo = FALSE, fig.cap = 'Bias and variance for the moving histogram. The animation shows how for small bandwidths the bias of $\\hat f_N(x;h)$ on estimating $f(x)$ is small, but the variance is high, and how for large bandwidths the bias is large and the variance is small. The variance is represented by the asymptotic $95\\%$ confidence intervals for $\\hat f_N(x;h)$. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/bias-var-movhist/).', screenshot.alt = "TODO.png", dev = 'png', cache = TRUE, out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/bias-var-movhist/', height = '900px')
```

Estimator \@ref(eq:movhist) raises an important question: **Why giving the same weight to all $X_1,\ldots,X_n$ in $(x-h, x+h)$?** After all, we are estimating $f(x)=F'(x)$ by estimating $\frac{F(x+h)-F(x-h)}{2h}$ through the relative frequency of $X_1,\ldots,X_n$ in $(x-h,x+h)$. Should not be the data points closer to $x$ more important than the ones further away? The answer to this question shows that \@ref(eq:movhist) is indeed a particular case of a wider class of density estimators. 

## Kernel density estimation {#dens-kde}

The moving histogram \@ref(eq:movhist) can be equivalently written as
\begin{align}
\hat f_N(x;h)&=\frac{1}{nh}\sum_{i=1}^n\frac{1}{2}1_{\{-h<x-X_i<h\}}\nonumber\\
&=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_i}{h}\right), (\#eq:movhist2)
\end{align}
with $K(z)=\frac{1}{2}1_{\{-1<z<1\}}$. Interestingly, $K$ is a uniform *density* in $(-1,1)$. This means that, when approximating
\begin{align*}
\mathbb{P}[x-h<X<x+h]=\mathbb{P}\left[-1<\frac{x-X}{h}<1\right]
\end{align*}
by \@ref(eq:movhist2), we give equal weight to all the . The generalization of \@ref(eq:movhist2) is now obvious: replace $K$ by an arbitrary density. Then $K$ is known as a *kernel*, a density that is typically symmetric and unimodal at $0$. This generalization provides the definition of **kernel density estimator** (kde):
\begin{align}
\hat f(x;h):=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_i}{h}\right). (\#eq:kde)
\end{align}
A common notation is $K_h(z):=\frac{1}{h}K\left(\frac{z}{h}\right)$, so $\hat f(x;h)=\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)$.

Several types of kernels are possible. The most popular is the *normal kernel* $K(z)=\phi(z)$, although the *Epanechnikov kernel*, $K(z)=\frac{3}{4}(1-z^2)1_{|z|<1}$, is slightly more efficient, as we will see later. The *rectangular kernel* $K(z)=\frac{1}{2}1_{\{|z|<1\}}$ yields the moving histogram as a particular case. The kde inherits the smoothness properties of the kernel. That means, for example, \@ref(eq:kde) with a normal kernel is infinitely differentiable. But with an Epanechnikov kernel, \@ref(eq:kde) is only first differentiable, and with a rectangular kernel is not even continuous. However, if a certain smoothness is guaranteed (continuity at least), the *choice of the kernel has little importance in practise* (at least compared with the choice of $h$). Figure \@ref(fig:kdeconst) illustrates the construction of the kde and the bandwidth and kernel effects.

```{r, kdeconst, echo = FALSE, fig.cap = 'Construction of the kernel density estimator. The animation shows how the bandwidth and kernel affect the density estimate, and how the kernels are rescaled densities with modes at the data points. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde/).', screenshot.alt = "TODO.png", dev = 'png', cache = TRUE, out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde/', height = '1000px')
```

Implementation of kde in `R` is built-in through the `density` function. The function automatically chooses the bandwidth $h$ using a *bandwith selector* which will be studied in detail in Section \@ref(dens-bwd).

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Sample 100 points from a N(0, 1)
set.seed(1234567)
samp <- rnorm(n = 100, mean = 0, sd = 1)

# Quickly compute a kde and plot the density object
# Automatically chooses bandwidth and uses normal kernel
plot(density(x = samp))

# Select a particular bandwidth (0.5) and kernel (Epanechnivok)
lines(density(x = samp, bw = 0.5, kernel = "epanechnikov"), col = 2)

# density automatically chooses the interval for plotting the kde
# (observe that the black line goes to roughly between -3 and 3)
# This can be tuned using "from" and "to"
plot(density(x = samp, from = -4, to = 4), xlim = c(-5, 5))

# The density object is a list
kde <- density(x = samp, from = -5, to = 5, n = 1024)
str(kde)
# Note that the evaluation grid "x"" is not directly controlled, only through 
# "from, "to", and "n" (better use powers of 2)
plot(kde$x, kde$y, type = "l")
curve(dnorm(x), col = 2, add = TRUE) # True density
rug(samp)
```

```{exercise}
Load the dataset `faithful`. Then:

- Estimate and plot the density of `faithful$eruptions`.
- Create a new plot and superimpose different density estimations with bandwidths equal to $0.1$, $0.5$, and $1$.
- Get the density estimate at *exactly* the point $x=3.1$ using $h=0.15$ and the Epanechnikov kernel.

```

## Asymptotic properties {#dens-kdeasymp}

Asymptotic results give us insights on the large-sample ($n\to\infty$) properties of an estimator. One might think why are they are useful, since in practice we only have *finite* sample sizes. Apart from purely theoretical reasons, asymptotic results usually give *highly valuable insights* on the properties of the method, typically simpler than finite-sample results (which might be analytically untractable). 

Along this section we will assume the following assumptions:

- **A1**. The density $f$ is twice continuously differentiable and square integrable.
- **A2**. The kernel $K$ is a symmetric and bounded pdf with finite fourth moment^[This implies that $\int z^pK(z)\mathrm{d}z<\infty$, with $0\leq p\leq 4$. Note that the odd moments are zero due to symmetry.].
- **A3**. $h=h_n$ is a deterministic sequence of bandwidths^[$h=h_n$ *always* depends on $n$ from now on, although the subscript is dropped for the ease of notation.] such that, when $n\to\infty$, $h\to0$ and $nh\to\infty$. 

We need to introduce some notation. From now on, the integrals are thought to be over $\mathbb{R}$, if not stated otherwise. The second moment of the kernel is denoted as $\mu_2(K):=\int z^2K(z)\mathrm{d}z$. The squared integral of a function $f$, is denoted by $R(f):=\int f(x)^2\mathrm{d}x$. The *convolution* between two real functions $f$ and $g$, $f*g$, is the function
\begin{align}
(f*g)(x):=\int f(x-y)g(y)\mathrm{d}y=(g*f)(x).(\#eq:conv)
\end{align}
We are now ready to obtain the bias and variance of $\hat f(x;h)$. Recall that is not possible to apply the "binomial trick" we used previously since now the estimator is not piecewise constant. Instead of that, we use the linearity of the kde and the convolution definition. For the bias, recall that
\begin{align}
\mathbb{E}[\hat f(x;h)]&=\frac{1}{n}\sum_{i=1}^n\mathbb{E}[K_h(x-X_i)]\nonumber\\
&=\int K_h(x-y)f(y)\mathrm{d}y\nonumber\\
&=(K_h * f)(x).(\#eq:kdekhbias)
\end{align}
Similarly, the variance is obtained as
\begin{align}
\mathbb{V}\mathrm{ar}[\hat f(x;h)]&=\frac{1}{n}((K_h^2*f)(x)-(K_h*f)^2(x)).(\#eq:kdekhvar)
\end{align}
These two expressions are exact, but they are hard to interpret. Equation \@ref(eq:kdekhbias) indicates that the estimator is *biased*, but it does not differentiate explicitly the effects of kernel, bandwidth and density on the bias. The same happens with \@ref(eq:kdekhvar), yet more emphasized. That is why the following *asymptotic* expressions are preferred.
```{theorem, label = "kdebiasvar"}
Under **A1**--**A3**, the bias and variance of the kde at $x$ are
\begin{align}
\mathrm{Bias}[\hat f(x;h)]&=\frac{\mu_2(K)}{2}f''(x)h^2+o(h^2),(\#eq:kdebias)\\
\mathbb{V}\mathrm{ar}[\hat f(x;h)]&=\frac{R(K)}{nh}f(x)+o((nh)^{-1}).(\#eq:kdevar)
\end{align}

```

```{proof}
For the *bias* we consider the change of variables $z=\frac{x-y}{h}$, $y=x-hz$, $\mathrm{d}y=-h\mathrm{d}z$. The integral limits flip and we have:
\begin{align}
\mathbb{E}[\hat f(x;h)]&=\int K_h(x-y)f(y)\mathrm{d}y\nonumber\\
&=\int K(z)f(x-hz)\mathrm{d}z.(\#eq:kdebias1)
\end{align}
Since $h\to0$, an applycation of Taylor's theorem gives
\begin{align}
f(x-hz)=&\,f(x)+hzf'(x)+\frac{1}{2}h^2z^2f''(x)\\
&+o(h^2z^2). (\#eq:kdebias2)
\end{align}
Substituting \@ref(eq:kdebias2) in \@ref(eq:kdebias1), and bearing in mind that $K$ is a symmetric density around $0$, we have
\begin{align*}
\int &K(z)f(x-hz)\mathrm{d}z\\
=&\,\int K(z)\big\{f(x)+hzf'(x)+\frac{1}{2}h^2z^2f''(x)
&+o(h^2z^2)\big\}\mathrm{d}z\\
=&\,f(x)+\frac{1}{2}h^2\mu_2(K)f''(x)+o(h^2)
\end{align*}
which provides \@ref(eq:kdebias). 

For the *variance*, first note that 
\begin{align}
\mathbb{V}\mathrm{ar}[\hat f(x;h)]&=\frac{1}{n^2}\sum_{i=1}^n\mathbb{V}\mathrm{ar}[K_h(x-X_i)]\nonumber\\
&=\frac{1}{n}\left\{\mathbb{E}[K_h^2(x-X)]-\mathbb{E}[K_h(x-X)]^2\right\}. (\#eq:kdevar1)
\end{align}
The second term of \@ref(eq:kdevar1) is already computed, so we focus on the first. Using the previous change of variables, we have:
\begin{align}
\mathbb{E}[K_h^2(x-X)]&=\frac{1}{h}\int K^2(z)f(x-hz)\mathrm{d}z\nonumber\\
&=\frac{1}{h}\int K^2(z)\left\{f(x)+O(hz)\right\}\mathrm{d}z\nonumber\\
&=\frac{R(K)}{h}f(x)+O(1). (\#eq:kdevar2)
\end{align}
Plugging-in \@ref(eq:kdevar1) into \@ref(eq:kdevar2) gives
\begin{align*}
\mathbb{V}\mathrm{ar}[\hat f(x;h)]&=\frac{1}{n}\left\{\frac{R(K)}{h}f(x)+O(1)-O(1)\right\}\\
&=\frac{R(K)}{nh}f(x)+o((nh)^{-1}),
\end{align*}
since $n^{-1}=o((nh)^{-1})$.
```

```{remark}
Integrating little-$o$'s is a delicate issue. In general, integrating a $o^x(1)$ quantity, possibly dependent on $x$, does not provide an $o(1)$. In other words: $\int o^x(1)\mathrm{d}x\neq o(1)$. If the previous hold with equality, then the limits and integral will be interchangeable. But this is not always true -- only if certains conditions are met; recall the dominated convergence Theorem (Theorem \@ref(thm:dct)). If one wants to be completely rigorous on the two implicit commutations of integrals and limits that took place in the proof, it is necessary to have explicit control of the remainder via Taylor's theorem (Theorem \@ref(thm:tay)) and then apply the dominated convergence Theorem. For simplicity in the exposition, we avoid this.
```

The bias and variance expressions \@ref(eq:kdebias) and \@ref(eq:kdevar) yield interesting insights (see Figure \@ref(fig:movinghist) for their visualization):

- The bias decreases with $h$ *quadratically*. In addition, the bias at $x$ is directly proportional to $f''(x)$. This has an interesting interpretation:

    - The bias is negative in concave regions, *i.e.* $\{x\in\mathbb{R}:f(x)''<0\}$. These regions correspod to *peaks and modes of $f$*, where the kde *underestimates* $f$ (tends to be below $f$). 
    - Conversely, the bias is positive in convex regions, *i.e.* $\{x\in\mathbb{R}:f(x)''>0\}$. These regions correspod to *valleys and tails of $f$*, where the kde *overestimates* $f$ (tends to be above $f$).
    - **The wilder the curvature $f''$, the harder to estimate $f$**. Flat density regions are easier to estimate than wiggling regions with high curvature (several modes).

- The variance depends directly on $f(x)$. **The higher the density, the more variable is the kde.** Interestingly, the variance decreases as a factor of $(nh)^{-1}$, a consequence of $nh$ playing the role of the *effective sample size* for estimating $f(x)$. The density at $x$ is not estimated using all the sample points, but only the fraction $nh$ in a neighborhood of $x$.

The MSE of the kde is trivial to obtain from the bias and variance:

```{corollary}
Under **A1**--**A3**, the MSE of the kde at $x$ is
\begin{align}
\mathrm{MSE}[\hat f(x;h)]=\,&\frac{1}{4}\mu^2_2(K)(f''(x))^2+\frac{R(K)}{nh}f(x)\nonumber\\
&+o(h^4+(nh)^{-1}).(\#eq:kdemse)
\end{align}
Therefore, the kde is pointwise consistent in MSE, *i.e.*, $\hat f(x;h)\stackrel{2}{\longrightarrow}f(x)$.
```
```{proof}
We apply that $\mathrm{MSE}[\hat f(x;h)]=(\mathbb{E}[\hat f(x;h)]-f(x))^2+\mathbb{V}\mathrm{ar}[\hat f(x;h)]$ and that $(O(h^2)+o(h^2))^2=O(h^4)+o(h^4)$. Since $\mathrm{MSE}[\hat f(x;h)]\to0$ when $n\to\infty$, consistency follows.
```

Finally, we prove the asymptotic pointwise normality of the estimator.

```{theorem, label = "point"}
Under **A1**--**A3**,
\begin{align}
&\sqrt{nh}(\hat f(x;h)-\mathbb{E}[\hat f(x;h)])\stackrel{d}{\longrightarrow}\mathcal{N}(0,R(K)f(x)),(\#eq:kdenorm1)\\
&\sqrt{nh}\left(\hat f(x;h)-f(x)-\frac{\mu_2(K)}{2}f''(x)h^2\right)\stackrel{d}{\longrightarrow}\mathcal{N}(0,R(K)f(x)).(\#eq:kdenorm2)
\end{align}

```
```{proof}
First note that $K_h(x-X_n)$ is a sequence of independendent but *not* identically distributed random variables: $h$ depends on $n$. Therefore, we look forward to apply Theorem \@ref(thm:lya).

We prove first \@ref(eq:kdenorm1). For simplicity, denote $K_i:=K_h(x-X_i)$, $i=1,\ldots,n$. From the proof of Theorem \@ref(thm:kdebiasvar) we know that $\mathbb{E}[K_i]=\mathbb{E}[\hat f(x;h)]=f(x)+o(1)$ and 
\begin{align*}
s_n^2&=\sum_{i=1}^n \mathbb{V}\mathrm{ar}[K_i]\\
&=n^2\mathbb{V}\mathrm{ar}[\hat f(x;h)]\\
&=n\frac{R(K)}{h}f(x)(1+o(1)).
\end{align*}
An application of the $C_p$ inequality (first) and Jensen's inequality (second), gives: 
$$
\mathbb{E}\left[|K_i+\mathbb{E}[K_i]|^{2+\delta}\right]\leq C_{2+\delta}\left(\mathbb{E}\left[|K_i|^{2+\delta}\right]+|\mathbb{E}[K_i]|^{2+\delta}\right)\leq 2C_{2+\delta}\mathbb{E}\left[|K_i|^{2+\delta}\right]=O\left(\mathbb{E}\left[|K_i|^{2+\delta}\right]\right).
$$
In addition, we have that by a Taylor expansion after $z=\frac{x-y}{h}$:
\begin{align*}
\mathbb{E}\left[|K_i|^{2+\delta}\right]&=\frac{1}{h^{2+\delta}}\int K^{2+\delta}\left(\frac{x-y}{h}\right)f(y)\mathrm{d}y\\
&=\frac{1}{h^{1+\delta}}\int K^{2+\delta}(z)f(x-hz)\mathrm{d}y\\
&=\frac{1}{h^{1+\delta}}\int K^{2+\delta}(z)(f(x)+o(1))\mathrm{d}y\\
&=O\left(h^{-(1+\delta)}\right).
\end{align*}
(Taking for example $\delta\leq2$ due to **A2**. The same previous remark on the remainder applies.)

Then:
\begin{align*}
\frac{1}{s_n^{2+\delta}}&\sum_{i=1}^n\mathbb{E}\left[|K_i-\mathbb{E}[\hat f(x;h)]|^{2+\delta}\right]\\
&=\left(\frac{h}{nR(K)f(x)}\right)^{1+\frac{\delta}{2}}(1+o(1))O\left(nh^{-(1+\delta)}\right)\\
&=O\left((nh)^{-\frac{\delta}{2}}\right)
\end{align*}
and the Lyapunov's condition is satisfied. As a consequence, \@ref(eq:kdenorm1) is proved.

To prove \@ref(eq:kdenorm2), we consider:
\begin{align*}
\sqrt{nh}&\left(\hat f(x;h)-f(x)-\frac{\mu_2(K)}{2}f''(x)h^2\right)\\
&=\sqrt{nh}(\hat f(x;h)-\mathbb{E}[\hat f(x;h)]+o(h^2)).
\end{align*}
Due to Example \@ref(exm:das) and Proposition \@ref(prp:ohps), we can prove that $\hat f(x;h)-\mathbb{E}[\hat f(x;h)]=o_\mathbb{P}(1)$. Then, $\hat f(x;h)-\mathbb{E}[\hat f(x;h)]+o(h^2)=(\hat f(x;h)-\mathbb{E}[\hat f(x;h)])(1+o_\mathbb{P}(h^2))$. Therefore, Slutsky's theorem and \@ref(eq:kdenorm2) give:
\begin{align*}
\sqrt{nh}&\left(\hat f(x;h)-f(x)-\frac{\mu_2(K)}{2}f''(x)h^2\right)\\
&=\sqrt{nh}(\hat f(x;h)-\mathbb{E}[\hat f(x;h)])(1+o_\mathbb{P}(h^2))\\
&\stackrel{d}{\longrightarrow}\mathcal{N}(0,R(K)f(x)).
\end{align*}

```

Note the rate $\sqrt{nh}$ in the asymptotic normality results. This is different from the standard CLT rate $\sqrt{n}$ (see Theorem \@ref(thm:clt)). It is *slower*: the variance of the limiting normal distribution decreases as $O((nh)^{-1})$ and not as $O(n^{-1})$. This phenomenon is related with the effective sample size used in the smoothing.

## Bandwidth selection {#dens-bwd}

As we saw in the previous sections, bandwidth selection is a key issue in density estimation. The purpose of this section is to introduce objective and automatic bandwidth selectors that attempt to minimize the estimation error of the target density $f$.

The first step is to define a global, rather than local, error criterion. The *Integrated Squared Error* (ISE),
$$
\mathrm{ISE}[\hat f(\cdot,h)]:=\int (\hat f(x;h)-f(x))^2\mathrm{d}x,
$$
is the squared distance between the kde and the target density. The ISE is a random quantity, since it depends directly on the sample $X_1,\ldots,X_n$. As a consequence, looking for an optimal-ISE bandwidth is a hard task, since it the optimality is dependent on the sample itself and not only on the population and $n$. To avoid this problematic, it is usual to compute the *Mean Integrated Squared Error* (MISE):
\begin{align*}
\mathrm{MISE}[\hat f(\cdot;h)]:=&\,\mathbb{E}\left[\mathrm{ISE}[\hat f(\cdot,h)]\right]\\
=&\,\mathbb{E}\left[\int (\hat f(x;h)-f(x))^2\mathrm{d}x\right]\\
=&\,\int \mathbb{E}\left[(\hat f(x;h)-f(x))^2\right]\mathrm{d}x\\
=&\,\int \mathrm{MSE}[\hat f(x;h)]\mathrm{d}x.
\end{align*}
The MISE is convenient due to its mathematical tractability and its natural relation with the MSE. There are, however, other error criteria that present attractive properties, such as the *Mean Integrated Absolute Error* (MIAE):
\begin{align*}
\mathrm{MIAE}[\hat f(\cdot;h)]:=&\,\mathbb{E}\left[\int |\hat f(x;h)-f(x)|\mathrm{d}x\right]\\
=&\,\int \mathbb{E}\left[|\hat f(x;h)-f(x)|^2\right]\mathrm{d}x.
\end{align*}
The MIAE, unless the MISE, is invariant with respect to monotone transformations of the data. For example, if $g(x)=f(t^{-1}(x))(t^{-1})'(x)$ is the density of $Y=t(X)$ and $X\sim f$, then if the change of variables $y=t(x)$ is made,
\begin{align*}
\int |\hat f(x;h)-f(x)|\mathrm{d}x&=\int |\hat f(t^{-1}(y);h)-f(t^{-1}(y))|)(t^{-1})'(y)\mathrm{d}y\\
&=\int |\hat g(y;h)-g(y)|\mathrm{d}y.
\end{align*}
Despite this appealing property, the analysis of MIAE is substantially more complicated. We refer to @Devroye1985 for a comprehensive treatment of absolute value metrics for kde.

Once the MISE is set as the error criterion to be minimized, our aim is to find
$$
h_\mathrm{MISE}=\arg\min_{h>0}\mathrm{MISE}[\hat f(\cdot;h)].
$$
For that purpose, we need an explicit expression of the MISE that we can attempt to minimize. An asymptotic expansion for the MISE can be easily derived from the MSE expression.

```{corollary}
Under **A1**--**A3**,
\begin{align}
\mathrm{MISE}[\hat f(\cdot;h)]=&\,\frac{1}{4}\mu^2_2(K)R(f'')h^4+\frac{R(K)}{nh}\nonumber\\
&+o(h^4+(nh)^{-1}).(\#eq:kdemise)
\end{align}
Therefore, $\mathrm{MISE}[\hat f(\cdot;h)]\to0$ when $n\to\infty$.
```
```{proof}
Trivial.
```

The dominating part of the MISE is denoted as AMISE, which stands for *Asymptotic MISE*: $\mathrm{AMISE}[\hat f(\cdot;h)]=\frac{1}{4}\mu^2_2(K)R(f'')h^4+\frac{R(K)}{nh}$. Due to its closed expression, it is possible to obtain a bandwidth that minimizes the AMISE.

```{corollary}
The bandwidth that minimises the AMISE is
$$
h_\mathrm{AMISE}=\left[\frac{R(K)}{\mu_2^2(K)R(f'')n}\right]^{1/5}.
$$
The optimal AMISE is: 
$$
\inf_{h>0}\mathrm{AMISE}[\hat f(\cdot;h)]=\frac{5}{4}(\mu_2^2(K)R(K)^4)^{4/5}R(f'')^{1/5}n^{-4/5}.
$$
```
```{proof}
Solving $\frac{\mathrm{d}}{\mathrm{d} h}\mathrm{AMISE}[\hat f(\cdot;h)]=0$, *i.e.* $\mu_2^2(K)R(f'')h^3- R(K)n^{-1}h^{-2}=0$, yields $h_\mathrm{AMISE}$ and $\mathrm{AMISE}[\hat f(\cdot;h_\mathrm{AMISE})]$ gives the AMISE-optimal error.

```

The AMISE-optimal order deserves some further inspection. It can be seen in Section 3.2 of @Scott2015 that the AMISE-optimal order for the *histogram* of Section \@ref(dens-hist) is $\left(\frac{3}{4}\right)^{2/3}R(f')^{1/3}n^{-2/3}$. Two facts are of interest. First, the MISE of the histogram is asymptotically larger than the MISE of the kde ($n^{-4/5}=o(n^{-2/3})$). This is a quantification of the quite apparent visual improvement of the kde over the histogram. Second, $R(f')$ appears instead of $R(f'')$, evidencing that the histogram is affected not only by the curvature of the target density $f$ but also by how fast it varies.

Unfortunately, the AMISE bandwidth depends on $R(f'')=\int(f''(x))^2\mathrm{d}x$, which measures the *curvature* of the density. As a consequence, it can not be readily applied in practice. In the next subsection we will see how to plug-in estimates for $R(f'')$.

### Plug-in rules {#dens-plug-in}

A simple solution to estimate $R(f'')$ is to assume that $f$ is the density of a $\mathcal{N}(\mu,\sigma^2)$, and then plug-in the form of the curvature for such density:
$$
R(\phi''_\sigma(\cdot-\mu))=\frac{3}{8\pi^{1/2}\sigma^5}.
$$
While doing so, we approximate the curvature of an arbitrary density by means of the curvature of a Normal. We have that
$$
h_\mathrm{AMISE}=\left[\frac{8\pi^{1/2}R(K)}{3\mu_2^2(K)n}\right]^{1/5}\sigma.
$$
Interestingly, the bandwidth is directly proportional to the standard deviation of the target density. Replacing $\sigma$ by an estimate yields the **normal scale bandwidth selector**, which we denote by $\hat h_\mathrm{NS}$ to emphasize its randomness:
$$
\hat h_\mathrm{NS}=\left[\frac{8\pi^{1/2}R(K)}{3\mu_2^2(K)n}\right]^{1/5}\hat\sigma.
$$
The estimate $\hat\sigma$ can be chosen as the standard deviation $s$, or, in order to avoid the effects of potential outliers, as the standardized interquantile range:
$$
\hat \sigma_{\mathrm{IQR}}:=\frac{X_{([0.75n])}-X_{([0.25n])}}{\Phi^{-1}(0.75)-\Phi^{-1}(0.25)}.
$$
@Silverman1986 suggests employing the minimum of both quantities, 
\begin{align}
\hat\sigma=\min(s,\hat \sigma_{\mathrm{IQR}}). (\#eq:sigmahat)
\end{align}
When combined with a normal kernel, for which $\mu_2(K)=1$ and $R(K)=\phi_{\sqrt{2}}(0)=\frac{1}{2\sqrt{\pi}}$ (check \@ref(eq:fact2)), this particularization of $\hat h_{\mathrm{NS}}$ gives the famous **rule-of-thumb** for bandwidth selection:
$$
\hat h_\mathrm{RT}=\left(\frac{4}{3}\right)^{1/5}n^{-1/5}\hat\sigma\approx1.06n^{-1/5}\hat\sigma.
$$

$\hat h_{\mathrm{RT}}$ is implemented in `R` through the function `bw.nrd` (not to confuse with `bw.nrd0`).
```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Data
x <- rnorm(100)

# Rule-of-thumb
bw.nrd(x = x)

# Same as
iqr <- diff(quantile(x, c(0.75, 0.25))) / diff(qnorm(c(0.75, 0.25)))
1.06 * length(x)^(-1/5) * min(sd(x), iqr)
```

The previous selector is an example of a **zero-stage plug-in** selector, a terminology which lays on the fact that $R(f'')$ was estimated by plugging-in a parametric assumption at the "very beginning". Because we could have opted to estimate $R(f'')$ nonparametrically and then plug-in the estimate into into $h_\mathrm{AMISE}$. Let's explore this possibility in more detail. But first, note the useful equality
$$
\int f^{(s)}(x)^2\mathrm{d}x=(-1)^s\int f^{(2s)}(x)f(x)\mathrm{d}x.
$$
This holds by a iterative application of integration by parts. For example, for $s=2$, take $u=f'(x)$ and $\mathrm{d}v=f''(x)\mathrm{d}x$. This gives
\begin{align*}
\int f''(x)^2\mathrm{d}x&=[f''(x)f'(x)]_{-\infty}^{+\infty}-\int f'(x)f'''(x)\mathrm{d}x\\
&=-\int f'(x)f'''(x)\mathrm{d}x
\end{align*}
under the assumption that the derivatives vanish at infinity. Applying again integration by parts with $u=f'(x)$ and $\mathrm{d}v=f''(x)\mathrm{d}x$ gives the result. This simple derivation has an important consequence: for estimating the functionals $R(f^{(s)})$ it is only required to estimate for $r=2s$ the functionals
$$
\psi_r:=\int f^{(r)}(x)f(x)\mathrm{d}x=\mathbb{E}[f^{(r)}(X)].
$$
Particularly, $R(f'')=\psi_4$.

Thanks to the previous expression, a possible way to estimate $\psi_r$ nonparametrically is
\begin{align}
\hat\psi_r(g)&=\frac{1}{n}\sum_{i=1}^n\hat f^{(r)}(X_i;g)\nonumber\\
&=\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^nL_g^{(r)}(X_i-X_j),(\#eq:psir)
\end{align}
where $\hat f^{(r)}(\cdot;g)$ is the $r$-th derivative of a kde with bandwidth $g$ and kernel $L$, *i.e.* $\hat f^{(r)}(x;g)=\frac{1}{nh^r}\sum_{i=1}^nL^{(r)}\left(\frac{x-X_i}{g}\right)$. Note that $g$ and $L$ can be different from $h$ and $K$, respectively. It turns out that estimating $\psi_r$ involves the adequate selection of a bandwidth $g$. The agenda is analogous to the one for $h_\mathrm{AMISE}$, but now taking into account that both $\hat\psi_r(g)$ and $\psi_r$ are *scalar* quantities:

1. Under certain regularity assumptions (see Section 3.5 in @Wand1995 for full details), the asymptotic bias and variance of $\hat\psi_r(g)$ are obtained. With them, we can compute the asymptotic expansion of the MSE and obtain the *Asymptotic Mean Squared Error* AMSE:
\begin{align*}
\mathrm{AMSE}[\hat \psi_r(r)]=&\,\left\{\frac{L^{(r)}(0)}{ng^{r+1}}+\frac{\mu_2(L)\psi_{r+2}g^2}{4}\right\}+\frac{2R(L^{(r)})\psi_0}{n^2g^{2r+1}}\\
&+\frac{4}{n}\left\{\int f^{(r)}(x)^2f(x)\mathrm{d}x-\psi_r^2\right\}.
\end{align*}
*Note*: $k$ is the highest integer such that $\mu_k(L)>0$. In these notes we have restricted to the case $k=2$ for the kernels $K$, but there are theoretical gains if one allows *high-order kernels* $L$ with vanishing even moments larger than $2$ for estimating $\psi_r$.

2. Obtain the AMSE-optimal bandwidth:
$$
g_\mathrm{AMSE}=\left[-\frac{k!L^{(r)}(0)}{\mu_k(L)\psi_{r+k}n}\right]^{1/(r+k+1)}
$$
The order of the optimal AMSE is 
$$\inf_{g>0}\mathrm{AMSE}[\hat \psi_r(r)]=\begin{cases}
O(n^{-(2k+1)/(r+k+1)}),&k<r,\\
O(n^{-1}),&k\geq r,
\end{cases}$$
which shows that a parametric-like rate of convergence can be achieved with high-order kernels. If we consider $L=K$ and $k=2$, then
$$
g_\mathrm{AMSE}=\left[-\frac{2K^{(r)}(0)}{\mu_2(L)\psi_{r+2}n}\right]^{1/(r+3)}.
$$

The above result has a major problem: it depends on $\psi_{r+2}$! Thus if we want to estimate $R(f'')=\psi_4$ by $\hat\psi_4(g_\mathrm{AMSE})$ we will need to estimate $\psi_6$. But $\hat\psi_6(g_\mathrm{AMSE})$ will depend on $\psi_8$ and so on. The solution to this convoluted problem is to stop estimating the functional $\psi_r$ after a given number $\ell$ of *stages*, therefore the terminology **$\ell$-stage plug-in selector**. At the $\ell$ stage, the functional $\psi_{\ell+4}$ in the AMSE-optimal bandwidth for estimating $\psi_{\ell+2}$ is computed assuming that the density is a $\mathcal{N}(\mu,\sigma^2)$, for which
$$
\psi_r=\frac{(-1)^{r/2}r!}{(2\sigma)^{r+1}(r/2)!\sqrt{\pi}},\quad \text{for }r\text{ even.}
$$
Typically, **two stages** are considered a good trade-off between bias (mitigated when $\ell$ increases) and variance (augments with $\ell$) of the plug-in selector. This is the method proposed by @Sheather1991, where they consider $L=K$ and $k=2$, yielding what we call the **direct plug-in** (DPI). The algorithm is:

1. Estimate $\psi_8$ using $\hat\psi_8^\mathrm{NS}:=\frac{105}{32\sqrt{\pi}\hat\sigma^9}$, where $\hat\sigma$ is given in \@ref(eq:sigmahat) 
2. Estimate $\psi_6$ using $\hat\psi_6(g_1)$ from \@ref(eq:psir), where 
$$
g_1:=\left[-\frac{2K^{(6)}(0)}{\mu_2(K)\hat\psi^\mathrm{NS}_{8}n}\right]^{1/9}.
$$
3. Estimate $\psi_4$ using $\hat\psi_4(g_2)$ from \@ref(eq:psir), where
$$
g_2:=\left[-\frac{2K^{(4)}(0)}{\mu_2(K)\hat\psi_6(g_1)n}\right]^{1/7}.
$$
4. The selected bandwidth is
$$
\hat h_{\mathrm{DPI}}:=\left[\frac{R(K)}{\mu_2^2(K)\hat\psi_4(g_2)n}\right]^{1/5}.
$$

```{remark}
The derivatives $K^{(r)}$ for the normal kernel can be obtained using *Hermite polynmials*: $\phi^{(r)}(x)=\phi(x)H_r(x)$. For $r=4,6$, $H_4(x)=x^4-6x^2+3$ and $H_6(x)=x^6-16x^4+45x^2-15$.
```

$\hat h_{\mathrm{DPI}}$ is implemented in `R` through the function `bw.SJ` (use `method = "dpi"`). The package `ks` provides `hpi`, which is a faster implementation that allows for more flexibility and has a somehow more complete documentation.
```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Data
x <- rnorm(100)

# Rule-of-thumb
bw.SJ(x = x, method = "dpi")

# Similar to
library(ks)
hpi(x) # Default is two-stages
```

### Cross-validation {#dens-cv}

We turn now our attention to a different philosophy of bandwidth estimation. Instead of trying to minimise the AMISE by plugging-in estimates for the unknown curvature term, we directly attempt to minimise the MISE by using the sample twice: one for computing the kde and other for *evalauting* its performance on estimating $f$. To avoid the clear dependence on the sample, we do this evaluation in a *cross-validatory* way: the data used for computing the kde is *not* used for its evaluation.

We begin by expanding the square in the MISE expression:
\begin{align*}
\mathrm{MISE}[\hat f(\cdot;h)]=&\,\mathbb{E}\left[\int (\hat f(x;h)-f(x))^2\mathrm{d}x\right]\\
=&\,\mathbb{E}\left[\int \hat f(x;h)^2\mathrm{d}x\right]-2\mathbb{E}\left[\int \hat f(x;h)f(x)\mathrm{d}x\right]\\
&+\int f(x)^2\mathrm{d}x.
\end{align*}
Since the last term does not depend on $h$, minimizing $\mathrm{MISE}[\hat f(\cdot;h)]$ is equivalent to minimizing
$$
\mathbb{E}\left[\int \hat f(x;h)^2\mathrm{d}x\right]-2\mathbb{E}\left[\int \hat f(x;h)f(x)\mathrm{d}x\right].
$$
This quantity is unknown, but it can be estimated unbiasedly (see Exercise \@ref(exr:lscv)) by
\begin{align}
\mathrm{LSCV}(h):=\int\hat f(x;h)^2\mathrm{d}x-2n^{-1}\sum_{i=1}^n\hat f_{-i}(X_i;h),(\#eq:lscv)
\end{align}
where $\hat f_{-i}(\cdot;h)$ is the *leave-one-out* kde and is based on the sample with the $X_i$ removed:
$$
\hat f_{-i}(x;h)=\frac{1}{n-1}\sum_{\substack{j=1\\j\neq i}}^n K_h(x-X_j).
$$
The motivation for \@ref(eq:lscv) is the following. The first term is unbiased by design. The second arises from approximating $\int \hat f(x;h)f(x)\mathrm{d}x$ by Monte Carlo, or in other words, by replacing $f(x)\mathrm{d}x=\mathrm{d}F(x)$ with $\mathrm{d}F_n(x)$. This gives
$$
\int \hat f(x;h)f(x)\mathrm{d}x\approx\frac{1}{n}\sum_{i=1}^n \hat f(X_i;h)
$$
and, in order to mitigate the dependence of the sample, we replace $\hat f(X_i;h)$ by $\hat f_{-i}(X_i;h)$ above. In that way, we use the sample for estimating the integral involving $\hat f(\cdot;h)$, but fo each $X_i$ we compute the kde on the *remaining* points. The **least squares cross-validation** (LSCV) selector, also denoted **unbiased cross-validation** (UCV) selector, is defined as
$$
\hat h_\mathrm{LSCV}:=\arg\min_{h>0}\mathrm{LSCV}(h).
$$
Numerical optimization is required for obtaining $\hat h_\mathrm{LSCV}$, contraty to the previous plug-in selectos, and there is little control on the shape of the objective function. This will be also the case for the forthcoming bandwidth selectors. The following remark warns about the dangers of numerical optimization in this context.

```{remark}
Numerical optimization of the LSCV function can be challenging. In practise, several local minima are possible, and the roughness of the objective function can vary notably depending on $n$ and $f$. As a consquence, optimization routines may get trapped in spurious solutions. To be on the safe side, it is always advisable to check the solution by plotting $\mathrm{LSCV}(h)$ for a range of $h$, or to perform a search in a bandwidth grid: $\hat h_\mathrm{LSCV}\approx\arg\min_{h_1,\ldots,h_G}\mathrm{LSCV}(h)$. 
```

$\hat h_{\mathrm{LSCV}}$ is implemented in `R` through the function `bw.ucv`. `bw.ucv` uses `R`'s `optimize`, which is quite sensible to the selection of the search interval (long intervals containing the solution may lead to unsatisfactory termination of the search; and short intervals might not contain the minimum). Therefore, some care is needed and that is why the `bw.ucv.mod` function is presented.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Data
set.seed(123456)
x <- rnorm(100)

# UCV gives a warning
bw.ucv(x = x)

# Extend search interval
args(bw.ucv)
bw.ucv(x = x, lower = 0.01, upper = 1)

# bw.ucv.mod replaces the optimization routine of bw.ucv by an exhaustive search on
# "h.grid" (chosen adaptatively from the sample) and optionally plots the LSCV curve
# with "plot.cv"
bw.ucv.mod <- function(x, nb = 1000L, 
                       h.grid = diff(range(x)) * (seq(0.1, 1, l = 200))^2, 
                       plot.cv = FALSE) {
  if ((n <- length(x)) < 2L) 
    stop("need at least 2 data points")
  n <- as.integer(n)
  if (is.na(n)) 
    stop("invalid length(x)")
  if (!is.numeric(x)) 
    stop("invalid 'x'")
  nb <- as.integer(nb)
  if (is.na(nb) || nb <= 0L) 
    stop("invalid 'nb'")
  storage.mode(x) <- "double"
  hmax <- 1.144 * sqrt(var(x)) * n^(-1/5)
  Z <- .Call(stats:::C_bw_den, nb, x)
  d <- Z[[1L]]
  cnt <- Z[[2L]]
  fucv <- function(h) .Call(stats:::C_bw_ucv, n, d, cnt, h)
  # h <- optimize(fucv, c(lower, upper), tol = tol)$minimum
  # if (h < lower + tol | h > upper - tol) 
  #   warning("minimum occurred at one end of the range")
  obj <- sapply(h.grid, function(h) fucv(h))
  h <- h.grid[which.min(obj)]
  if (plot.cv) {
    plot(h.grid, obj, type = "o")
    rug(h.grid)
    abline(v = h, col = 2, lwd = 2)
  }
  h
}

# Compute the bandwidth and plot the LSCV curve
bw.ucv.mod(x = x, plot.cv = TRUE)

# We can compare with the default bw.ucv output
abline(v = bw.ucv(x = x), col = 3)
```

The next cross-validation selector is based on **biased cross-validation** (BCV). The BCV selector presents a hybrid strategy that combines plug-in and cross-validation ideas. It starts by considering the AMISE expression in \@ref(eq:kdemise)
$$
\mathrm{AMISE}[\hat f(\cdot;h)]=\frac{1}{4}\mu^2_2(K)R(f'')h^4+\frac{R(K)}{nh}
$$
and then plugs-in an estimate for $R(f'')$ based on a modification of $R(\hat{f}''(\cdot;h))$. The modification is 
\begin{align}
\widetilde{R(f'')}:=&\,R(\hat{f}''(\cdot;h))-\frac{R(K'')}{nh^5}\nonumber\\
=&\,\frac{1}{n}\sum_{i=1}^n\sum_{\substack{j=1\\j\neq i}}^n(K_h''*K_h'')(X_i-X_j)(\#eq:Rtilde),
\end{align}
a *leave-out-diagonals* estimate of $R(f'')$. It is designed to reduce the bias of $R(\hat{f}''(\cdot;h))$, since 
$\mathbb{E}\left[R(\hat{f}''(\cdot;h))\right]=R(f'')+\frac{R(K'')}{nh^5}+O(h^2)$. [@Scott1987]. Plugging-in \@ref(eq:Rtilde) into the AMISE expression yields the BCV objective function and BCV bandwidth selector:
\begin{align*}
\mathrm{BCV}(h)&:=\frac{1}{4}\mu^2_2(K)\widetilde{R(f'')}h^4+\frac{R(K)}{nh},\\
\hat h_\mathrm{BCV}&:=\arg\min_{h>0}\mathrm{BCV}(h).
\end{align*}

The appealing property of $\hat h_\mathrm{BCV}$ is that it has a considerably smaller variance compared to $\hat h_\mathrm{LSCV}$. This reduction in variance comes at the price of an increased bias, which tends to make $\hat h_\mathrm{BCV}$ larger than $h_\mathrm{MISE}$.

$\hat h_{\mathrm{BCV}}$ is implemented in `R` through the function `bw.bcv`. Again, `bw.bcv` uses `R`'s `optimize` so the `bw.bcv.mod` function is presented to have better guarantees on finding the adequate minimum.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Data
set.seed(123456)
x <- rnorm(100)

# BCV gives a warning
bw.bcv(x = x)

# Extend search interval
args(bw.bcv)
bw.bcv(x = x, lower = 0.01, upper = 1)

# bw.bcv.mod replaces the optimization routine of bw.bcv by an exhaustive search on
# "h.grid" (chosen adaptatively from the sample) and optionally plots the BCV curve
# with "plot.cv"
bw.bcv.mod <- function(x, nb = 1000L, 
                       h.grid = diff(range(x)) * (seq(0.1, 1, l = 200))^2, 
                       plot.cv = FALSE) {
  if ((n <- length(x)) < 2L) 
    stop("need at least 2 data points")
  n <- as.integer(n)
  if (is.na(n)) 
    stop("invalid length(x)")
  if (!is.numeric(x)) 
    stop("invalid 'x'")
  nb <- as.integer(nb)
  if (is.na(nb) || nb <= 0L) 
    stop("invalid 'nb'")
  storage.mode(x) <- "double"
  hmax <- 1.144 * sqrt(var(x)) * n^(-1/5)
  Z <- .Call(stats:::C_bw_den, nb, x)
  d <- Z[[1L]]
  cnt <- Z[[2L]]
  fbcv <- function(h) .Call(stats:::C_bw_bcv, n, d, cnt, h)
  # h <- optimize(fbcv, c(lower, upper), tol = tol)$minimum
  # if (h < lower + tol | h > upper - tol) 
  #   warning("minimum occurred at one end of the range")
  obj <- sapply(h.grid, function(h) fbcv(h))
  h <- h.grid[which.min(obj)]
  if (plot.cv) {
    plot(h.grid, obj, type = "o")
    rug(h.grid)
    abline(v = h, col = 2, lwd = 2)
  }
  h
}

# Compute the bandwidth and plot the BCV curve
bw.bcv.mod(x = x, plot.cv = TRUE)

# We can compare with the default bw.bcv output
abline(v = bw.bcv(x = x), col = 3)
```

### Comparison of bandwidth selectors {#dens-comp}

We state next some insights from the convergence results of the DPI, LSCV, and BCV selectors. All of them are based in results of the kind 
$$
n^\nu(\hat h/h_\mathrm{MISE}-1)\stackrel{d}{\longrightarrow}\mathcal{N}(0,\sigma^2),
$$
where $\sigma^2$ depends only on $K$ and $f$, and measures how variable is the selector. The rate $n^\nu$ serves to quantify how fast the relative $\hat h/h_\mathrm{MISE}-1$ decreases (the larger the $\nu$, the faster the convergence). Under certain regularity conditions, we have::

- $n^{1/10}(\hat h_\mathrm{LSCV}/h_\mathrm{MISE}-1)\stackrel{d}{\longrightarrow}\mathcal{N}(0,\sigma_\mathrm{LSCV}^2)$ and $n^{1/10}(\hat h_\mathrm{BCV}/h_\mathrm{MISE}-1)\stackrel{d}{\longrightarrow}\mathcal{N}(0,\sigma_\mathrm{BCV}^2)$. Both selectors have a slow rate of convergence (compare it with the $n^{1/2}$ of the CLT). Inspection of the variances of both selectors reveals that, for the normal kernel $\sigma_\mathrm{LSCV}^2/\sigma_\mathrm{BCV}^2\approx 15.7$. Therefore, LSCV is considerably more variable than BCV.

- $n^{-5/14}(\hat h_\mathrm{DPI}/h_\mathrm{MISE}-1)\stackrel{d}{\longrightarrow}\mathcal{N}(0,\sigma_\mathrm{DPI}^2)$. Thus, the DPI selector has a convergence rate much faster than the cross-validation selectors. There is an appealing explanation for this phenomenon. Recall that $\hat h_\mathrm{BCV}$ minimices the slightly modified version of $\mathrm{BCV}(h)$ given by
$$
\frac{1}{4}\mu_2^2(K)\tilde\psi_4(h)h^4+\frac{R(K)}{nh}
$$
and
\begin{align*}
\tilde\psi_4(h):=&\frac{1}{n(n-1)}\sum_{i=1}^n\sum_{\substack{j=1\\j\neq i}}^n(K_h''*K_h'')(X_i-X_j)\nonumber\\
=&\frac{n}{n-1}\widetilde{R(f'')}.
\end{align*}
$\tilde\psi_4$ is a leave-out-diagonals estimate of $\psi_4$. Despite being different from $\hat\psi_4$, it serves for building a DPI analogous to BCV points towards the precise fact that drags down the performance of BCV. The modified version of the DPI minimises
$$
\frac{1}{4}\mu_2^2(K)\tilde\psi_4(g)h^4+\frac{R(K)}{nh},
$$
where $g$ is independent of $h$. The two methods differ on the the way $g$ is chosen: BCV sets $g=h$ and the modified DPI looks for the best $g$ in terms of the $\mathrm{AMSE}[\tilde\psi_4(g)]$. It can be seen that $g_\mathrm{AMSE}=O(n^{-2/13})$, whereas the $h$ used in BCV is asymptotically $O(n^{-1/5})$. This suboptimality on the choice of $g$ is the reason of the asymptotic deficiency of BCV.

We focus now on exploring the empirical performance of bandwidth selectors. The workhorse for doing that is simulation. A popular collection of simulation scenarios was given by @Marron1992 and are conveniently available through the package `nor1mix`. They forma collection of normal $r$-mixtures of the form
$$
f(x;\boldsymbol{\mu},\boldsymbol{\Sigma},\mathbf{w}):=\sum_{j=1}^rw_j\phi_{\sigma_j}(x-\mu_j),\quad  w_j\geq0,\quad j=1,\ldots.r,\quad \sum_{j=1}^rw_j=1.
$$
Densities of this form are specially attractive since they allow for arbitrarily flexibility and, if the normal kernel is employed, they allow for *explicit and exact* MISE expressions:
\begin{align}
\mathrm{MISE}_r[\hat f(\cdot;h)]&=(2\sqrt{\pi}nh)^{-1}+\mathbf{w}'\{(1-n^{-1})\boldsymbol{\Omega}_2-2\boldsymbol{\Omega}_1+\boldsymbol{\Omega}_0\}\mathbf{w},\nonumber\\ (\boldsymbol{\Omega}_a)_{ij}&=\phi_{(ah^2+\sigma_i^2+\sigma_j^2)^{1/2}}(\mu_i-\mu_j),\quad i,j=1,\ldots,r.(\#eq:misenorm)
\end{align}

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Load package
library(nor1mix)

# Available models
?MarronWand

# Simulating
samp <- rnorMix(n = 500, obj = MW.nm9) # MW object in the second argument
hist(samp, freq = FALSE)

# Density evaluation
x <- seq(-4, 4, length.out = 400)
lines(x, dnorMix(x = x, obj = MW.nm9), col = 2)

# Plot a MW object directly 
# A normal with the same mean and variance is plotted in dashed lines
par(mfrow = c(2, 2))
plot(MW.nm5)
plot(MW.nm7)
plot(MW.nm10)
plot(MW.nm12)
lines(MW.nm10) # Also possible
```

Figure \@ref(fig:kdebwd) presents a visualization of the performance of the kde with different bandwidth selectors, carried out in the family of mixtures of @Marron1992.

```{r, kdebwd, echo = FALSE, fig.cap = 'Performance comparison of bandwidth selectors. The RT, DPI, LSCV, and BCV are computed for each sample for a normal mixture density. For each sample, computes the ISEs of the selectors and sorts them from best to worst. Changing the scenarios gives insight on the adequacy of each selector to hard- and simple-to-estimate densities. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde-bwd/).', screenshot.alt = "TODO.png", dev = 'png', cache = TRUE, out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde-bwd/', height = '950px')
```

## Confidence intervals {#dens-ci}

Obtaining a confidence interval (CI) for $f(x)$ is a hard task. Due to the bias results seen in Section \@ref(dens-kdeasymp), we know that the kde is biased for finite sample sizes, $\mathbb{E}[\hat f(x;h)]=(K_h*f)(x)$, and it is only *asymptotically* unbiased when $h\to0$. This bias is called the **smoothing bias** and in essence complicates the obtention of CIs for $f(x)$, but not of $(K_h*f)(x)$. Let's see with an illustrative example the differences between these two objects.

Two well-known facts for normal densities (see Appendix C in @Wand1995) are:
\begin{align}
(\phi_{\sigma_1}(\cdot-\mu_1)*\phi_{\sigma_2}(\cdot-\mu_2))(x)&=\phi_{(\sigma_1^2+\sigma_2^2)^{1/2}}(x-\mu_1-\mu_2),(\#eq:fact1)\\
\int\phi_{\sigma_1}(x-\mu_1)\phi_{\sigma_2}(x-\mu_2)\mathrm{d}x&=\phi_{(\sigma_1^2+\sigma_2^2)^{1/2}}(\mu_1-\mu_2),\\
\phi_{\sigma}(x-\mu)^r&=\frac{1}{\sigma^{r-1}}(2\pi)^{(1-r)/2}\phi_{\sigma/ r^{1/2}}(x-\mu)\frac{1}{r^{1/2}}.(\#eq:fact2)
\end{align}
As a consequence, if $K=\phi$ (*i.e.*, $K_h=\phi_h$) and $f(\cdot)=\phi_\sigma(\cdot-\mu)$, we have that
\begin{align}
(K_h*f)(x)&=\phi_{(h^2+\sigma^2)^{1/2}}(x-\mu),(\#eq:normbias)\\
(K_h^2*f)(x)&=\left(\frac{1}{(2\pi)^{1/2}h}\phi_{h/2^{1/2}}/2^{1/2}*f\right)(x)\nonumber\\
&=\frac{1}{2\pi^{1/2}h}\left(\phi_{h/2^{1/2}}*f\right)(x)\nonumber\\
&=\frac{1}{2\pi^{1/2}h}\phi_{(h^2/2+\sigma^2)^{1/2}}(x-\mu).(\#eq:normbias2)
\end{align}
Thus, the *exact* expectation of the kde for estimating the density of a $\mathcal{N}(\mu,\sigma^2)$ is the density of a $\mathcal{N}(\mu,\sigma^2+h^2)$. Clearly, when $h\to0$, the bias disappears (at expenses of increasing the variance, of course). Removing this finite-sample size bias is not simple: if the bias is expanded, $f''$ appears. Thus for attempting to unbias $\hat f(\cdot;h)$ we have to estimate $f''$, which is much more complicated than estimating $f$! Taking second derivatives on the kde does not simply work out-of-the-box, since the bandwidths for estimating $f$ and $f''$ scale differently. We refer the interested reader to Section 2.12 in @Wand1995 for a quick review of derivative kde.

The previous deadlock can be solved if we limit our ambitions. Rather than constructing a confidence interval for $f(x)$, we will do it for $\mathbb{E}[\hat f(x;h)]=(K_h*f)(x)$. There is nothing wrong with this as long as we are careful when we report the results to make it clear that the CI is for $(K_h*f)(x)$ and not $f(x)$. 

The building block for the CI for $\mathbb{E}[\hat f(x;h)]=(K_h*f)(x)$ is Theorem \@ref(thm:point), which stated that:
$$
\sqrt{nh}(\hat f(x;h)-\mathbb{E}[\hat f(x;h)])\stackrel{d}{\longrightarrow}\mathcal{N}(0,R(K)f(x)).
$$
Plugging-in $\hat f(x;h)=f(x)+O_\mathbb{P}(h^2+(nh)^{-1})=f(x)(1+o_\mathbb{P}(1))$ (see Exercise \@ref(exr:fop)) as an estimate for $f(x)$ in the variance, we have by the Slutsky's theorem that
$$
\sqrt{\frac{nh}{R(K)\hat f(x;h)}}(\hat f(x;h)-\mathbb{E}[\hat f(x;h)])=\sqrt{\frac{nh}{R(K)f(x)}}(\hat f(x;h)-\mathbb{E}[\hat f(x;h)])(1+o_\mathbb{P}(1))\stackrel{d}{\longrightarrow}\mathcal{N}(0,1).
$$
Therefore, an **asymptotic $100(1-\alpha)\%$ confidence interval for $\mathbb{E}[\hat f(x;h)]$** that can be straightforwardly computed is:
\begin{align}
I=\left(\hat f(x;h)\pm z_{\alpha}\sqrt{\frac{R(K)\hat f(x;h)}{nh}}\right).(\#eq:ci)
\end{align}
Recall that if we wanted to do obtain a CI with the second result in Theorem \@ref(thm:point) we will need to estimate $f''(x)$.

```{remark}
Several points regarding the CI require proper awareness:

1. The CI is for $\mathbb{E}[\hat f(x;h)]=(K_h*f)(x)$, not $f(x)$.
2. The CI is **pointwise**. That means that $\mathbb{P}\left[\mathbb{E}[\hat f(x;h)]\in I\right]\approx 1-\alpha$ for each $x\in\mathbb{R}$, but **not simultaneously**. This is, $\mathbb{P}\left[\mathbb{E}[\hat f(x;h)]\in I,\,\forall x\in\mathbb{R}\right]\neq 1-\alpha$.
3. We are approximating $f(x)$ in the variance by $\hat f(x;h)=f(x)+O_\mathbb{P}(h^2+(nh)^{-1})$. Additionally, the convergence to a normal distribution happens at rate $\sqrt{nh}$. Hence both $h$ and $nh$ need to small and large, respectively, for a good coverage.
4. The CI is for a **deterministic bandwidth** $h$ (*i.e.*, not selected from the sample), which is not usually the case in practise. If a bandwidth selector is employed, the coverage will be affected, specially for small $n$.

```

We illustrate the construction of the \@ref(eq:ci) in for the situation where the reference density is a $\mathcal{N}(\mu,\sigma^2)$ and the normal kernel is employed. This allows to use \@ref(eq:normbias)
and \@ref(eq:normbias2) in combination \@ref(eq:kdekhbias) and \@ref(eq:kdekhvar) with to obtain:
\begin{align*}
\mathbb{E}[\hat f(x;h)]&=\phi_{(h^2+\sigma^2)^{1/2}}(x-\mu),\\
\mathbb{V}\mathrm{ar}[\hat f(x;h)]&=\frac{1}{n}\left(\frac{\phi_{(h^2/2+\sigma^2)^{1/2}}(x-\mu)}{2\pi^{1/2}h}-(\phi_{(h^2+\sigma^2)^{1/2}}(x-\mu))^2\right).
\end{align*}

The following piece of code evaluates the proportion of times that $\mathbb{E}[\hat f(x;h)]$ belongs to $I$ for each $x\in\mathbb{R}$, both estimating and knowing the variance in the asymptotic distribution.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# R(K) for a normal
Rk <- 1 / (2 * sqrt(pi))

# Generate a sample from a N(mu, sigma^2)
n <- 100
mu <- 0
sigma <- 1
set.seed(123456)
x <- rnorm(n = n, mean = mu, sd = sigma)

# Compute the kde (NR bandwidth)
kde <- density(x = x, from = -4, to = 4, n = 1024, bw = "nrd")

# Selected bandwidth
h <- kde$bw

# Estimate the variance
hatVarKde <- kde$y * Rk / (n * h)

# True expectation and variance (because the density is a normal)
EKh <- dnorm(x = kde$x, mean = mu, sd = sqrt(sigma^2 + h^2))
varKde <- (dnorm(kde$x, mean = mu, sd = sqrt(h^2 / 2 + sigma^2)) / 
             (2 * sqrt(pi) * h) - EKh^2) / n

# CI with estimated variance
alpha <- 0.05
zalpha <- qnorm(1 - alpha/2)
ciLow1 <- kde$y - zalpha * sqrt(hatVarKde)
ciUp1 <- kde$y + zalpha * sqrt(hatVarKde)

# CI with known variance
ciLow2 <- kde$y - zalpha * sqrt(varKde)
ciUp2 <- kde$y + zalpha * sqrt(varKde)

# Plot estimate, CIs and expectation
plot(kde, main = "Density and CIs", ylim = c(0, 1))
lines(kde$x, ciLow1, col = "gray")
lines(kde$x, ciUp1, col = "gray")
lines(kde$x, ciUp2, col = "gray", lty = 2)
lines(kde$x, ciLow2, col = "gray", lty = 2)
lines(kde$x, EKh, col = "red")
legend("topright", legend = c("Estimate", "CI estimated var", 
                              "CI known var", "Smoothed density"), 
       col = c("black", "gray", "gray", "red"), lwd = 2, lty = c(1, 1, 2, 1))
```

The above code computes the ci. But it dos not show any insight on the effective coverage of the ci. The next simulation exercise deals with this issue.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Simulation setting
n <- 100; h <- 0.2
mu <- 0; sigma <- 1 # Normal parameters
M <- 5e2 # Number of replications in the simulation
nGrid <- 512 # Number of x's for computing the kde
alpha <- 0.05; zalpha <- qnorm(1 - alpha/2) # alpha for CI

# Compute expectation and variance
kde <- density(x = 0, bw = h, from = -4, to = 4, n = nGrid) # Just to get kde$x
EKh <- dnorm(x = kde$x, mean = mu, sd = sqrt(sigma^2 + h^2))
varKde <- (dnorm(kde$x, mean = mu, sd = sqrt(h^2 / 2 + sigma^2)) / 
           (2 * sqrt(pi) * h) - EKh^2) / n

# For storing if the mean is inside the CI
insideCi1 <- insideCi2 <- matrix(nrow = M, ncol = nGrid)

# Simulation
set.seed(12345)
for (i in 1:M) {
  
  # Sample & kde
  x <- rnorm(n = n, mean = mu, sd = sigma)
  kde <- density(x = x, bw = h, from = -4, to = 4, n = nGrid)
  hatSdKde <- sqrt(kde$y * Rk / (n * h))
  
  # CI with estimated variance
  ciLow1 <- kde$y - zalpha * hatSdKde
  ciUp1 <- kde$y + zalpha * hatSdKde
  
  # CI with known variance
  ciLow2 <- kde$y - zalpha * sqrt(varKde)
  ciUp2 <- kde$y + zalpha * sqrt(varKde)
  
  # Check if for each x the mean is inside the CI
  insideCi1[i, ] <- EKh > ciLow1 & EKh < ciUp1
  insideCi2[i, ] <- EKh > ciLow2 & EKh < ciUp2
  
}

# Plot results
plot(kde$x, colMeans(insideCi1), ylim = c(0.25, 1), type = "l", 
     main = "Coverage CIs", xlab = "x", ylab = "Coverage")
lines(kde$x, colMeans(insideCi2), col = 4)
abline(h = 1 - alpha, col = 2)
abline(h = 1 - alpha + c(-1, 1) * qnorm(0.975) * 
         sqrt(alpha * (1 - alpha) / M), col = 2, lty = 2)
legend(x = "bottom", legend = c("CI estimated var", "CI known var", 
                                "Nominal level", 
                                "95% CI for the nominal level"),
       col = c(1, 4, 2, 2), lwd = 2, lty = c(1, 1, 1, 2))
```

```{exercise}
Explore the coverage of the asymptotic CI for varying values of $h$. To that end, adapt the previous coe to workin a `manipulate` environment like the example given below.
```

```{r, echo = TRUE, eval = FALSE, collapse = TRUE, cache = TRUE}
# Load manipulate
# install.packages("manipulate")
library(manipulate) 

# Sample
x <- rnorm(100)

# Simple plot of kde for varying h
manipulate({
  
  kde <- density(x = x, from = -4, to = 4, bw = h)
  plot(kde, ylim = c(0, 1), type = "l", main = "")
  curve(dnorm(x), from = -4, to = 4, col = 2, add = TRUE)
  rug(x)

}, h = slider(min = 0.01, max = 2, initial = 0.5, step = 0.01))
```

## Practical issues {#dens-prac}

We discuss in this section several practical issues for kernel density estimation.

### Boundary issues and transformations {#dens-transf}

In Section \@ref(dens-kdeasymp) we assumed certain regularity conditions for $f$. Assumption **A1** stated that $f$ should be twice continuously differentiable (on $\mathbb{R}$). It is simple to think a counterexample for that: take any density with bounded support, for example an $\mathrm{Exp}(1)$ in $(0,\infty)$, as seen below.

```{r, echo = TRUE, eval = FALSE, collapse = TRUE, cache = TRUE}
# Sample from an Exp(1)
set.seed(1234567)
samp <- rexp(n = 500)

# kde and density
plot(density(samp), ylim = c(0, 1))
curve(dexp(x), from = -2, to = 10, col = 2, add = TRUE)
rug(samp)
```

What is happening is clear: the kde is spreading probability mass outside the support of $\mathrm{Exp}(1)$, because the kernels are functions defined in $\mathbb{R}$. Since the kde places probability mass at negative values, it takes it from the positive side, resulting in a severe negative bias around $0$. As a consequence, the kde does not integrate one in the support of the data. No matter what is the sample size considered, the kde will always have a **negative bias of $O(h)$ at the boundary**, instead of the standard $O(h^2)$.

A simple approach to deal with the *boundary bis* is to map a compactly-supported density $f$ into a real-supported density $g$, that is simpler to estimate, by means of a transformation $t$:
$$
f(x)=g(t(x))t'(x).
$$
The *transformation kde* is obtained by replacing $g$ by the usual kde:
\begin{align}
\hat f_T(x;h,t):=\frac{1}{n}\sum_{i=1}^nK_h(t(x)-t(X_i))t'(x). (\#eq:tkde)
\end{align}
Note that $h$ is in the scale of $t(X_i)$, not $X_i$. Hence another plus of this approach is that bandwidth selection can be done transparently in terms of the previously seen bandwidth selectors applied to $t(X_1),\ldots,t(X_n)$. A table with some common transformations is:

| Transformation |      Useful for    |   $t(x)$   |    $t'(x)$    |
|:--------------:|:------------------:|:----------:|:-------------:|
| Log | Data in $(a,\infty)$, $a\in\mathbb{R}$   | $\log(x-a)$  | $\frac{1}{x-a}$ |
| Probit | Data in $(a,b)$   | $\Phi\left(\frac{x-a}{b-a}\right)$  | $\frac{1}{b-a}\phi\left(\frac{x-a}{b-a}\right)$ |
| Shifted power | Heavily skewed data in $\mathbb{R}$ and above $-\lambda_1$  | $\begin{cases}(x+\lambda_1)^{\lambda_2}\mathrm{sign}(\lambda_2),&\lambda_2\neq0,\\\log(x+\lambda_1),&\lambda_2=0.\end{cases}$ | $\begin{cases}\lambda_2(x+\lambda_1)^{\lambda_2-1}\mathrm{sign}(\lambda_2),&\lambda_2\neq0,\\\frac{1}{x+\lambda_1},&\lambda_2=0.\end{cases}$ |

The code below illustrates how to compute a transformation kde in practise.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Sample from an Exp(1)
set.seed(1234567)
samp <- rlnorm(n = 500)

# kde with log-transformed data
kde <- density(log(samp))
plot(kde, main = "kde of transformed data")
rug(log(samp))

# Careful: kde$x is in the reals!
range(kde$x)

# Untransform kde$x so the grid is in (0, infty)
kdeTransf <- kde
kdeTransf$x <- exp(kdeTransf$x)

# Transform the density using the chain rule
kdeTransf$y <- kdeTransf$y * 1 / kdeTransf$x

# Transformed kde
plot(kdeTransf, main = "Transformed kde", xlim = c(0, 3))
curve(dlnorm(x), col = 2, add = TRUE)
rug(samp)
```

<!-- App transformation -->

<!-- ```{r, kdetransf, echo = FALSE, fig.cap = '.Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde-transf/).', screenshot.alt = "TODO.png", dev = 'png', cache = TRUE, out.width = '90%'} -->
<!-- knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kde-transf/', height = '900px') -->
<!-- ``` -->

### Sampling {#dens-sampling}

Sampling a kde is relatively straightforward. The trick is to recall
$$
\hat f(x;h)=\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)
$$
as a mixture density made of $n$ mixture components in which each of them is sampled independently. The only part that might require special treatment is sampling from the density $K$, although for most of the implemented kernels `R` contains specific sampling functions. 

The algorithm for sampling $N$ points goes as follows:

1. Choose $i\in\{1,\ldots,n\}$ at random.
2. Sample from $K_h(\cdot-X_i)=\frac{1}{h}K\left(\frac{\cdot-X_i}{h}\right)$.
3. Repeat the previous steps $N$ times.

Let's see a quick application.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Sample the Claw
n <- 100
set.seed(23456)
samp <- rnorMix(n = n, obj = MW.nm10)

# Kde
h <- 0.1
plot(density(samp, bw = h))

# Naive sampling algorithm
sampKde <- numeric(1e6)
for (k in 1:1e6) {
  
  i <- sample(x = 1:100, size = 1)
  sampKde[k] <- rnorm(n = 1, mean = samp[i], sd = h)
  
}

# Add kde of the sampled kde - almost equal
lines(density(sampKde), col = 2)

# Sample 1e6 points from the kde
i <- sample(x = 100, size = 1e6, replace = TRUE)
sampKde <- rnorm(1e6, mean = samp[i], sd = h)

# Add kde of the sampled kde - almost equal
lines(density(sampKde), col = 3)

```

<!-- ## Multivariate extension {#dens-mult} -->

<!-- Setting and estimator.  -->

<!-- Product Kernels.  -->

<!-- Assumptions. Bias and variance. Asymptotic normality. No Taylor -->

<!-- Bandwidth selectors. Normal. UCV. BCV. PI. -->

<!-- Implementation. -->

<!-- ```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE} -->
<!-- #  -->
<!-- ``` -->

## Exercises {#dens-exercises}

This is the list of evaluable exercises for Chapter \@ref(dens). The number of stars represents an estimate of their difficulty: easy ($\star$), medium ($\star\star$), and hard ($\star\star\star$).

```{exercise}
(theoretical, $\star$) Prove that the histogram \@ref(eq:hist) is a proper density estimate (a nonnegative density that integrates one). Obtain its associated distribution function. What is its difference with respect to the ecdf \@ref(eq:ecdf)?
```

```{exercise}
(theoretical, $\star$, adapted from Exercise 2.1 in @Wand1995) Derive the result \@ref(eq:kdekhvar). Then obtain the *exact* MSE and MISE using \@ref(eq:kdekhbias) and \@ref(eq:kdekhvar).
```

```{exercise}
(theoretical, $\star\star$) Conditionally on the sample $X_1,\ldots,X_n$, compute the expectation and variance of the kde \@ref(eq:kde) and compare them with the sample mean and variance. What is the effect of $h$ in them?
```

```{exercise, label = "lscv"}
(theoretical, $\star\star$, Exercise 3.3 in @Wand1995) Show that
$$
\mathbb{E}[\mathrm{LSCV}(h)]=\mathrm{MISE}[\hat f(\cdot;h)]-R(f).
$$
```

```{exercise, label = "fop"}
(theoretical, $\star\star\star$) Show that:

- $\hat f(x;h)=f(x)+o(h^2)+O_\mathbb{P}\left((nh)^{-1/2}\right)=f(x)(1+o_\mathbb{P}(1))$.
- $\frac{1}{n}\sum_{i=1}^n(x-X_i)K_h(x-X_i)=\mu_2(K)f'(x)h^2+o(h^2)+O_\mathbb{P}\left(n^{-1/2}h^{\frac{1}{2}}\right)$.

Then, provide a similar expansion result for $\frac{1}{n}\sum_{i=1}^n(x-X_i)^2K_h(x-X_i)$. *Hint*: use Chebychev inequality.
```

```{exercise}
(theoretical, $\star\star\star$, Exercise 2.23 in @Wand1995) Show that the bias and variance for the transformation kde \@ref(eq:tkde) are
\begin{align*}
\mathrm{Bias}[\hat f_T(x;h,t)]&=\frac{\mu_2(K)}{2}g''(t(x))t'(x)h^2+o(h^2),\\
\mathbb{V}\mathrm{ar}[\hat f_T(x;h,t)]&=\frac{R(K)}{nh}g(t(x))t'(x)^2+o((nh)^{-1}),
\end{align*}
where $g$ is the density of $t(X)$. Usig these results, prove that
\begin{align*}
\mathrm{AMISE}[\hat f_T(\cdot;h,t)]=\frac{1}{4}h^4\mu_2^2(K)\int t'(t^{-1}(x))g''(x)^2\mathrm{d}x+\frac{R(K)}{nh}\mathbb{E}[t'(X)].
\end{align*}

```

```{exercise}
(practical, $\star$) TODO. 
<!-- Faithful bivariate. -->
```

```{exercise}
(practical, $\star$) The kde can be used to smoothly resample a dataset. To that end, first cumpute the kde of the dataset and then employ the algorithm of Section \@ref(dens-prac). Implement this resampling as a function that takes as arguments the dataset, the bandwidth $h$, and the number of sampled points $M$ wanted from the dataset. Use the normal kernel for simplicity. Test the implementation with the `faithful` dataset and different bandwidths.
```

```{exercise}
(practical, $\star\star$, Exercise 6.5 in @Wasserman2006)
Data on the salaries of the chief executive officer of 60 companies is available at <http://lib.stat.cmu.edu/DASL/Datafiles/ceodat.html>. Investigate the distribution of salaries using a kde. Use $\hat h_\mathrm{LSCV}$ to choose the amount of smoothing. Also consider $\hat h_\mathrm{RT}$. There appear to be a few bumps in the density. Are they real? Use confidence bands to address this question. Finally, comment on the resulting estimates.
```

```{exercise}
(practical, $\star\star$) Implement the transformation kde \@ref(eq:tkde) for the three transformations given in Section \@ref(dens-prac). You can tweak the output of the `density` function in `R` and add an extra argument for selecting the kind of transformation. Or you can implement it directly from scratch. 
```

```{exercise}
(practical, $\star\star\star$) A bandwidth selector is a random variable. Visualizing its density can help to understand its behaviour, especially if it is compared with the asymptotic optimal bandwidth $h_\mathrm{AMISE}$. Create a script that does the following steps:

1. For $j=1,\ldots,M=10000$:

    - Simulates a sample from a model mixture of `nor1mix`.
    - Computes the bandwidth selectors $\hat h_{\mathrm{RT}}$, $\hat h_{\mathrm{BCV}}$, $\hat h_{\mathrm{UCV}}$, and $\hat h_{\mathrm{DPI}}$, and stores them.
    
2. Estimates the density of each bandwidth selector from its corresponding sample of size $M$. Use the RT selctor for estimating the density.
3. Plots the estimated densities together.
4. Draws a vertical line for representing the $h_\mathrm{AMISE}$ bandwidth

Describe the results for the "Claw" and "Bimodal" densities in `nor1mix`, for sample sizes $n=100,500$.
```

```{exercise}
(practical, $\star\star\star$) Use \@ref(eq:misenorm) and the family of densities of @Marron1992 in `nor1mix` to compare the MISE and AMISE criteria. To that purpose, code \@ref(eq:misenorm) and the AMISE expression for the normal kernel and compare the two error curves and the two minimizers. Explore three models of your choice from `nor1mix` for sample sizes $n=50, 100, 200$. Describe in detail the results and the major takeaways.

```

