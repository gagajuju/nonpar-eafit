# Regression estimation {#reg}

The relation of two random variables $X$ and $Y$ can be completely characterized by their joint cdf $F$, or equivalently, by the joint pdf $f$ if $(X,Y)$ is continuous, the case we will address. In the regression setting, we are interested in predicting/explaining the *response* $Y$ by means of the *predictor* $X$ from a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$. The role of the variables is not symmetric: $X$ is *used* to predict/explain $Y$.

The complete knowledge of $Y$ when $X=x$ is given by the conditional pdf: $f_{Y\vert X=x}(y)=\frac{f(x,y)}{f_X(x)}$. While this pdf provides full knowledge about $Y\vert X=x$, it is also a challenging task to estimate it: for each $x$ we have to estimate a *curve*! A simpler approach, yet still challenging, is to estimate the conditional mean (a scalar) for each $x$. This is the so-called *regression function*^[Recall that we assume that $(X,Y)$ is continuous.]
$$
m(x):=\mathbb{E}[Y\vert X=x]=\int y\mathrm{d}F_{Y\vert X=x}(y)=\int yf_{Y\vert X=x}(y)\mathrm{d}y.
$$
Thus we aim to provide information about $Y$'s expectation, not distribution, by $X$.

Finally, recall that $Y$ can expressed in terms of $m$ by means of the *location-scale model*:
$$
Y=m(X)+\sigma(X)\varepsilon,
$$
where $\sigma^2(x):=\mathbb{V}\mathrm{ar}[Y\vert X=x]$ and $\varepsilon$ is independent from $X$ and such that $\mathbb{E}[\varepsilon]=0$ and $\mathbb{V}\mathrm{ar}[\varepsilon]=1$.

## Review on parametric regression {#reg-param}

We review now a couple of useful parametric regression models that will be used in the construction of nonparametric regression models.

### Linear regression {#reg-lin}

#### Model formulation and least squares {-}

The multiple linear regression employs *multiple* predictors $X_1,\ldots,X_p$^[Not to confuse with a sample!] for explaining a single response $Y$ by *assuming* that a linear relation of the form
\begin{align}
Y=\beta_0+\beta_1 X_1+\ldots+\beta_p X_p+\varepsilon (\#eq:lm)
\end{align}
holds between the predictors $X_1,\ldots,X_p$ and the response $Y$. In \@ref(eq:lm), $\beta_0$ is the *intercept* and $\beta_1,\ldots,\beta_p$ are the *slopes*, respectively. $\varepsilon$ is a random variable with mean zero and independent from $X_1,\ldots,X_p$. Another way of looking at \@ref(eq:lm) is
\begin{align}
\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]=\beta_0+\beta_1x_1+\ldots+\beta_px_p, (\#eq:lmexp)
\end{align}
since $\mathbb{E}[\varepsilon|X_1=x_1,\ldots,X_p=x_p]=0$. Therefore, the mean of $Y$ is changing in a *linear* fashion with respect to the values of $X_1,\ldots,X_p$. Hence the interpretation of the coefficients:

- $\beta_0$: is the mean of $Y$ when $X_1=\ldots=X_p=0$.
- $\beta_j$, $1\leq j\leq p$: is the **additive** increment in mean of $Y$ for an increment of one unit in $X_j=x_j$, provided that the remaining variables *do not change*.

Figure \@ref(fig:multmarg) illustrates the geometrical interpretation of a multiple linear model: a plane in the $(p+1)$-dimensional space. If $p=1$, the plane is the regression line for simple linear regression. If $p=2$, then the plane can be visualized in a three-dimensional plot. TODO: add another figure.

(ref:multmargtitle) The regression plane (blue) of $Y$ on $X_1$ and $X_2$, and its relation with the regression lines (green lines) of $Y$ on $X_1$ (left) and $Y$ on $X_2$ (right). The red points represent the sample for $(X_1,X_2,Y)$ and the black points the subsamples for $(X_1,X_2)$ (bottom), $(X_1,Y)$ (left) and $(X_2,Y)$ (right). Note that the regression plane is not a direct extension of the marginal regression lines.

```{r, multmarg, echo = FALSE, fig.cap = '(ref:multmargtitle)', dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '70%'}
knitr::include_graphics("images/R/multmarg.png")
```

The estimation of $\beta_0,\beta_1,\ldots,\beta_p$ is done by minimizing the so-called *Residual Sum of Squares* (RSS). First we need to introduce some helpful matrix notation:

- A sample of $(X_1,\ldots,X_p,Y)$ is denoted by $(X_{11},\ldots,X_{1p},Y_1),\ldots,(X_{n1},\ldots,X_{np},Y_n)$, where $X_{ij}$ denotes the $i$-th observation of the $j$-th predictor $X_j$. We denote with $\mathbf{X}_i=(X_{i1},\ldots,X_{ip})$ to the $i$-th observation of $(X_1,\ldots,X_p)$, so the sample simplifies to $(\mathbf{X}_{1},Y_1),\ldots,(\mathbf{X}_{n},Y_n)$.

- The *design matrix* contains all the information of the predictors and a column of ones
$$
\mathbf{X}=\begin{pmatrix}
1 & X_{11} & \cdots & X_{1p}\\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \cdots & X_{np}
\end{pmatrix}_{n\times(p+1)}.
$$

- The *vector of responses* $\mathbf{Y}$, the *vector of coefficients* $\boldsymbol\beta$ and the *vector of errors* are, respectively,
$$
\mathbf{Y}=\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix}_{n\times 1},\quad\boldsymbol\beta=\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{pmatrix}_{(p+1)\times 1},\text{ and }
\boldsymbol\varepsilon=\begin{pmatrix}
\varepsilon_1 \\
\vdots \\
\varepsilon_n
\end{pmatrix}_{n\times 1}.
$$
Thanks to the matrix notation, we can turn the sample version of the multiple linear model, namely
\begin{align*}
Y_i&=\beta_0 + \beta_1 X_{i1} + \ldots +\beta_p X_{ik} + \varepsilon_i,\quad i=1,\ldots,n,
\end{align*}
into something as compact as
\begin{align*}
\mathbf{Y}=\mathbf{X}\boldsymbol\beta+\boldsymbol\varepsilon.
\end{align*}

The RSS for the multiple linear regression is
\begin{align}
\text{RSS}(\boldsymbol\beta):=&\,\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_{i1}-\ldots-\beta_pX_{ik})^2\nonumber\\
=&\,(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}).(\#eq:rss)
\end{align}
The RSS aggregates the *squared vertical distances* from the data to a regression plane given by $\boldsymbol\beta$. Note that the *vertical distances* are considered because we want to minimize the error in the *prediction* of $Y$. Thus, the treatment of the variables is *not symmetrical*^[If that was the case, we would consider perpendicular distances, which yield to Principal Component Analysis (PCA).]; see Figure \@ref(fig:leastsquares2). The least squares estimators are the minimizers of the RSS:
\begin{align*}
\hat{\boldsymbol{\beta}}:=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}} \text{RSS}(\boldsymbol{\beta}).
\end{align*}
Luckily, thanks to the matrix form of \@ref(eq:rss), it is simple to compute a closed-form expression for the least squares estimates:
\begin{align}
\hat{\boldsymbol{\beta}}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}.(\#eq:ls)
\end{align}

```{exercise, label = "ls"}
$\hat{\boldsymbol{\beta}}$ can be obtained differentiating \@ref(eq:rss). Prove it using that $\frac{\partial \mathbf{A}\mathbf{x}}{\partial \mathbf{x}}=\mathbf{A}$ and $\frac{\partial f(\mathbf{x})'g(\mathbf{x})}{\partial \mathbf{x}}=f(\mathbf{x})'\frac{\partial g(\mathbf{x})}{\partial \mathbf{x}}+g(\mathbf{x})'\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}$ for two vector-valued functions $f$ and $g$.
```

(ref:leastsquares2title) The least squares regression plane $y=\hat\beta_0+\hat\beta_1x_1+\hat\beta_2x_2$ and its dependence on the kind of squared distance considered. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares-3D/).

```{r, leastsquares2, echo = FALSE, fig.cap = '(ref:leastsquares2title)', screenshot.alt = "images/screenshots/least-squares-3D.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '100%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares-3D/', height = '700px')
```

Let's check that indeed the coefficients given by `R`'s `lm` are the ones given by \@ref(eq:ls) in a toy linear model.
```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# Create the data employed in Figure 3.1

# Generates 50 points from a N(0, 1): predictors and error
set.seed(34567)
x1 <- rnorm(50)
x2 <- rnorm(50)
x3 <- x1 + rnorm(50, sd = 0.05) # Make variables dependent
eps <- rnorm(50)

# Responses
yLin <- -0.5 + 0.5 * x1 + 0.5 * x2 + eps
yQua <- -0.5 + x1^2 + 0.5 * x2 + eps
yExp <- -0.5 + 0.5 * exp(x2) + x3 + eps

# Data
dataAnimation <- data.frame(x1 = x1, x2 = x2, yLin = yLin,
                            yQua = yQua, yExp = yExp)

# Call lm
# lm employs formula = response ~ predictor1 + predictor2 + ...
# (names according to the data frame names) for denoting the regression
# to be done
mod <- lm(yLin ~ x1 + x2, data = dataAnimation)
summary(mod)

# mod is a list with a lot of information
# str(mod) # Long output

# Coefficients
mod$coefficients

# Application of formula (3.4)

# Matrix X
X <- cbind(1, x1, x2)

# Vector Y
Y <- yLin

# Coefficients
beta <- solve(t(X) %*% X) %*% t(X) %*% Y
beta
```

```{exercise}
Compute $\boldsymbol{\beta}$ for the regressions `yLin ~ x1 + x2`, `yQua ~ x1 + x2`, and `yExp ~ x2 + x3` using equation \@ref(eq:ls) and the function `lm`. Check that the fitted plane and the coefficient estimates are coherent.
```

Once we have the least squares estimates $\hat{\boldsymbol{\beta}}$, we can define the next two concepts:

- The *fitted values* $\hat Y_1,\ldots,\hat Y_n$, where
\begin{align*}
\hat Y_i:=\hat\beta_0+\hat\beta_1X_{i1}+\cdots+\hat\beta_pX_{ik},\quad i=1,\ldots,n.
\end{align*}
They are the vertical projections of $Y_1,\ldots,Y_n$ into the fitted line (see Figure \@ref(fig:leastsquares2)). In a matrix form, inputting \@ref(eq:rss)
$$
\hat{\mathbf{Y}}=\mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}=\mathbf{H}\mathbf{Y},
$$
where $\mathbf{H}:=\mathbf{X}(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'$ is called the *hat matrix* because it "puts the hat into $\mathbf{Y}$". What it does is to project $\mathbf{Y}$ into the regression plane (see Figure \@ref(fig:leastsquares2)).

- The *residuals* (or estimated errors) $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$, where
\begin{align*}
\hat\varepsilon_i:=Y_i-\hat Y_i,\quad i=1,\ldots,n.
\end{align*}
They are the vertical distances between actual data and fitted data.

#### Model assumptions {-}

Up to now, we have not made any probabilistic assumption on the data generation process. $\hat{\boldsymbol{\beta}}$ was derived from purely geometrical arguments, not probabilistic ones. However, some probabilistic assumptions are required for inferring the *unknown* population coefficients $\boldsymbol{\beta}$ from the sample $(\mathbf{X}_1, Y_1),\ldots,(\mathbf{X}_n, Y_n)$.

```{r, linearmodel2, echo = FALSE, fig.pos = 'h!', out.width = '70%', fig.cap = 'The key concepts of the simple linear model. The blue densities denote the conditional density of $Y$ for each cut in the $X$ axis. The yellow band denotes where the $95\\%$ of the data is, according to the model. The red points represent data following the model.', cache = TRUE}
knitr::include_graphics("images/R/linearmodel.png")
```

The assumptions of the multiple linear model are:

i. **Linearity**: $\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]=\beta_0+\beta_1x_1+\ldots+\beta_px_p$.
ii. **Homoscedasticity**: $\mathbb{V}\text{ar}[\varepsilon_i]=\sigma^2$, with $\sigma^2$ constant for $i=1,\ldots,n$.
iii. **Normality**: $\varepsilon_i\sim\mathcal{N}(0,\sigma^2)$ for $i=1,\ldots,n$.
iv. **Independence of the errors**: $\varepsilon_1,\ldots,\varepsilon_n$ are independent (or uncorrelated, $\mathbb{E}[\varepsilon_i\varepsilon_j]=0$, $i\neq j$, since they are assumed to be normal).

A good one-line summary of the linear model is the following (independence is assumed)
\begin{align}
Y|(X_1=x_1,\ldots,X_p=x_p)\sim \mathcal{N}(\beta_0+\beta_1x_1+\ldots+\beta_px_p,\sigma^2).(\#eq:linsum)
\end{align}
Inference on the parameters $\boldsymbol\beta$ and $\sigma$ can be done, conditionally^[We assume that the randomness is on the response only.] on $X_1,\ldots,X_n$, from \@ref(eq:linsum). We do not explore this further, referring the interested reader to [these notes](https://bookdown.org/egarpor/SSS2-UC3M/multlin-inference.html). Instead of that, we remark the connection between least squares estimation and the maximum likelihood estimator derived from \@ref(eq:linsum).

First, note that \@ref(eq:linsum) is the *population version* of the linear model (it is expressed in terms of the random variables, not in terms of their samples). The *sample version* that summarizes assumptions i--iv is
$$
\mathbf{Y}|(\mathbf{X}_1,\ldots,\mathbf{X}_p)\sim \mathcal{N}_n(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I}).
$$
Using this result, it is easy obtain the log-likelihood function of $Y_1,\ldots,Y_n$ conditionally^[We assume that the randomness is on the response only.] on $\mathbf{X}_1,\ldots,\mathbf{X}_n$ as
\begin{align}
\ell(\boldsymbol{\beta})=\log\phi_{\sigma^2\mathbf{I}}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})=\sum_{i=1}^n\log\phi_{\sigma}(Y_i-(\mathbf{X}\boldsymbol{\beta})_i).(\#eq:ell)
\end{align}
Finally, the next result justifies the consideration of the least squares estimate: it equals the maximum likelihood estimator derived under assumptions i--iv.

```{theorem, label = "lik"}
Under assumptions i--iv, the maximum likelihood estimate of $\boldsymbol{\beta}$ is the least squares estimate \@ref(eq:ls):
$$
\hat{\boldsymbol{\beta}}_\mathrm{ML}=\arg\max_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}\ell(\boldsymbol{\beta})=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}\mathbf{Y}.
$$
```
```{proof}
Expanding the first equality at \@ref(eq:ell) gives ($|\sigma^2\mathbf{I}|^{1/2}=\sigma^{n}$)
$$
\ell(\boldsymbol{\beta})=-\log((2\pi)^{n/2}\sigma^n)-\frac{1}{2\sigma^2}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}).
$$
Optimizing $\ell$ does not require knowledge on $\sigma^2$, since differentiating with respect to $\boldsymbol{\beta}$ and equating to zero gives (see Exercise \@ref(exr:ls)) $\frac{1}{\sigma^2}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'\mathbf{X}=0$. The result follows from that.
```

### Logistic regression {#reg-log}

#### Model formulation {-}

When the response $Y$ can take only two values, codified for convenience as $1$ (success) and $0$ (failure), it is called a *binary* variable. A binary variable, known also as a *Bernoulli variable*, is a $\mathrm{B}(1, p)$. Recall that $\mathbb{E}[\mathrm{B}(1, p)]=\mathbb{P}[\mathrm{B}(1, p)=1]=p$.

If $Y$ is a binary variable and $X_1,\ldots,X_p$ are predictors associated to $Y$, the purpose in *logistic regression* is to estimate
\begin{align}
p(x_1,\ldots,x_p):=&\,\mathbb{P}[Y=1|X_1=x_1,\ldots,X_p=x_p]\nonumber\\
=&\,\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p],(\#eq:expp)
\end{align}
this is, how the probability of $Y=1$ is changing according to particular values, denoted by $x_1,\ldots,x_p$, of the predictors $X_1,\ldots,X_p$. A tempting possibility is to consider a linear model for \@ref(eq:expp), $p(x_1,\ldots,x_p)=\beta_0+\beta_1x_1+\ldots+\beta_px_p$. However, such a model will run into serious problems inevitably: negative probabilities and probabilities larger than one will arise.

A solution is to consider a function to encapsulate the value of $z=\beta_0+\beta_1x_1+\ldots+\beta_px_p$, in $\mathbb{R}$, and map it back to $[0,1]$. There are several alternatives to do so, based on distribution functions $F:\mathbb{R}\longrightarrow[0,1]$ that deliver $y=F(z)\in[0,1]$. Different choices of $F$ give rise to different models, the most common being the *logistic distribution function*:
\begin{align*}
\mathrm{logistic}(z):=\frac{e^z}{1+e^z}=\frac{1}{1+e^{-z}}.
\end{align*}
Its inverse, $F^{-1}:[0,1]\longrightarrow\mathbb{R}$, known as the *logit function*, is
\begin{align*}
\mathrm{logit}(p):=\mathrm{logistic}^{-1}(p)=\log\frac{p}{1-p}.
\end{align*}
This is a *link function*, this is, a function that maps a given space (in this case $[0,1]$) into $\mathbb{R}$. The term link function is employed in *generalized linear models*, which follow exactly the same philosophy of the logistic regression -- mapping the domain of $Y$ to $\mathbb{R}$ in order to apply there a linear model. As said, different link functions are possible, but we will concentrate here exclusively on the logit as a link function.

The *logistic model* is defined as the following parametric form for \@ref(eq:expp):
\begin{align}
p(x_1,\ldots,x_p)&=\mathrm{logistic}(\beta_0+\beta_1x_1+\ldots+\beta_px_p)\nonumber\\
&=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\ldots+\beta_px_p)}}.(\#eq:eq-log)
\end{align}
The linear form inside the exponent has a clear interpretation:

- If $\beta_0+\beta_1x_1+\ldots+\beta_px_p=0$, then $p(x_1,\ldots,x_p)=\frac{1}{2}$ ($Y=1$ and $Y=0$ are equally likely).
- If $\beta_0+\beta_1x_1+\ldots+\beta_px_p<0$, then $p(x_1,\ldots,x_p)<\frac{1}{2}$ ($Y=1$ less likely).
- If $\beta_0+\beta_1x_1+\ldots+\beta_px_p>0$, then $p(x_1,\ldots,x_p)>\frac{1}{2}$ ($Y=1$ more likely).

To be more precise on the interpretation of the coefficients $\beta_0,\ldots,\beta_p$ we need to introduce the concept of *odds*. The **odds is an equivalent way of expressing the distribution of probabilities in a binary variable**. Since $\mathbb{P}[Y=1]=p$ and $\mathbb{P}[Y=0]=1-p$, both the success and failure probabilities can be inferred from $p$. Instead of using $p$ to characterize the distribution of $Y$, we can use
\begin{align}
\mathrm{odds}(Y)=\frac{p}{1-p}=\frac{\mathbb{P}[Y=1]}{\mathbb{P}[Y=0]}.(\#eq:eq-odds)
\end{align}
The odds is the *ratio between the probability of success and the probability of failure*. It is extensively used in betting due to its better interpretability. For example, if a horse $Y$ has a probability $p=2/3$ of winning a race ($Y=1$), then the odds of the horse is
$$
\text{odds}=\frac{p}{1-p}=\frac{2/3}{1/3}=2.
$$
This means that the horse has a *probability of winning that is twice larger than the probability of losing*. This is sometimes written as a $2:1$ or $2 \times 1$ (spelled "two-to-one"). Conversely, if the odds of $Y$ is given, we can easily know what is the probability of success $p$, using the inverse of \@ref(eq:eq-odds):
$$
p=\mathbb{P}[Y=1]=\frac{\text{odds}(Y)}{1+\text{odds}(Y)}.
$$
For example, if the odds of the horse were $5$, that would correspond to a probability of winning $p=5/6$.

```{remark}
Recall that the odds is a number in $[0,+\infty]$. The $0$ and $+\infty$ values are attained for $p=0$ and $p=1$, respectively. The log-odds (or logit) is a number in $[-\infty,+\infty]$.
```
We can rewrite \@ref(eq:eq-log) in terms of the odds \@ref(eq:eq-odds). If we do so, we have:
\begin{align*}
\mathrm{odds}(Y|&X_1=x_1,\ldots,X_p=x_p)\\
&=\frac{p(x_1,\ldots,x_p)}{1-p(x_1,\ldots,x_p)}\\
&=e^{\beta_0+\beta_1x_1+\ldots+\beta_px_p}\\
&=e^{\beta_0}e^{\beta_1x_1}\ldots e^{\beta_px_p}.
\end{align*}
This provides the following interpretation of the coefficients:

- $e^{\beta_0}$: is the odds of $Y=1$ when $X_1=\ldots=X_p=0$.
- $e^{\beta_j}$, $1\leq j\leq k$: is the **multiplicative** increment of the odds for an increment of one unit in $X_j=x_j$, provided that the remaining variables *do not change*. If the increment in $X_j$ is of $r$ units, then the multiplicative increment in the odds is $(e^{\beta_j})^r$.

#### Model assumptions and estimation {-}

Some probabilistic assumptions are required for performing inference on the model parameters $\boldsymbol\beta$ from the sample $(\mathbf{X}_1, Y_1),\ldots,(\mathbf{X}_n, Y_n)$. These assumptions are somehow simpler than the ones for linear regression.

```{r, logisticmodel, echo = FALSE, fig.pos = 'h!', out.width = '70%', fig.cap = 'The key concepts of the logistic model. The blue bars represent the conditional distribution of probability of $Y$ for each cut in the $X$ axis. The red points represent data following the model.', cache = TRUE}
knitr::include_graphics("images/R/logisticmodel.png")
```

The assumptions of the logistic model are the following:

i. **Linearity in the logit**^[An equivalent way of stating this assumption is $p(\mathbf{x})=\mathrm{logistic}(\beta_0+\beta_1x_1+\ldots+\beta_px_p)$.]: $\mathrm{logit}(p(\mathbf{x}))=\log\frac{
p(\mathbf{x})}{1-p(\mathbf{x})}=\beta_0+\beta_1x_1+\ldots+\beta_px_p$.
ii. **Binariness**: $Y_1,\ldots,Y_n$ are binary variables.
iii. **Independence**: $Y_1,\ldots,Y_n$ are independent.

A good one-line summary of the logistic model is the following (independence is assumed)
\begin{align*}
Y|(X_1=x_1,\ldots,X_p=x_p)&\sim\mathrm{Ber}\left(\mathrm{logistic}(\beta_0+\beta_1x_1+\ldots+\beta_px_p)\right)\\
&=\mathrm{Ber}\left(\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\ldots+\beta_px_p)}}\right).
\end{align*}

Since $Y_i\sim \mathrm{Ber}(p(\mathbf{X}_i))$, $i=1,\ldots,n$, the log-likelihood of $\boldsymbol{\beta}$ is
\begin{align}
\ell(\boldsymbol{\beta})=&\,\sum_{i=1}^n\log\left(p(\mathbf{X}_i)^{Y_i}(1-p(\mathbf{X}_i))^{1-Y_i}\right)\nonumber\\
=&\,\sum_{i=1}^n\left\{Y_i\log(p(\mathbf{X}_i))+(1-Y_i)\log(1-p(\mathbf{X}_i))\right\}.(\#eq:lllogist)
\end{align}
Unfortunately, due to the non-linearity of the optimization problem there are no explicit expressions for $\hat{\boldsymbol{\beta}}$. These have to be obtained numerically by means of an iterative procedure, which may run into problems in low sample situations with perfect classification. Unlike in the linear model, inference is not exact from the assumptions, but approximate in terms of maximum likelihood theory. We do not explore this further and refer the interested reader to [these notes](https://bookdown.org/egarpor/SSS2-UC3M/logreg-inference.html).

Figure \@ref(fig:maximumlikelihood) shows how the log-likelihood changes with respect to the values for $(\beta_0,\beta_1)$ in three data patterns.

(ref:maximumlikelihoodtitle) The logistic regression fit and its dependence on $\beta_0$ (horizontal displacement) and $\beta_1$ (steepness of the curve). Recall the effect of the sign of $\beta_1$ in the curve: if positive, the logistic curve has an 's' form; if negative, the form is a reflected 's'. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/log-maximum-likelihood/).

```{r, maximumlikelihood, echo = FALSE, fig.cap = '(ref:maximumlikelihoodtitle)', screenshot.alt = "images/screenshots/log-maximum-likelihood.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '100%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/log-maximum-likelihood/', height = '900px')
```

The data of the illustration has been generated with the following code:

Let's check that indeed the coefficients given by `R`'s `glm` are the ones that maximize the likelihood of the animation of Figure \@ref(fig:maximumlikelihood). We do so for `y ~ x1`.
```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# Create the data employed in Figure 3.4

# Data
set.seed(34567)
x <- rnorm(50, sd = 1.5)
y1 <- -0.5 + 3 * x
y2 <- 0.5 - 2 * x
y3 <- -2 + 5 * x
y1 <- rbinom(50, size = 1, prob = 1 / (1 + exp(-y1)))
y2 <- rbinom(50, size = 1, prob = 1 / (1 + exp(-y2)))
y3 <- rbinom(50, size = 1, prob = 1 / (1 + exp(-y3)))

# Data
dataMle <- data.frame(x = x, y1 = y1, y2 = y2, y3 = y3)

# Call glm
# glm employs formula = response ~ predictor1 + predictor2 + ...
# (names according to the data frame names) for denoting the regression
# to be done. We need to specify family = "binomial" to make a
# logistic regression
mod <- glm(y1 ~ x, family = "binomial", data = dataMle)
summary(mod)

# mod is a list with a lot of information
# str(mod) # Long output

# Coefficients
mod$coefficients

# Plot the fitted regression curve
xGrid <- seq(-5, 5, l = 200)
yGrid <- 1 / (1 + exp(-(mod$coefficients[1] + mod$coefficients[2] * xGrid)))
plot(xGrid, yGrid, type = "l", col = 2, xlab = "x", ylab = "y")
points(x, y1)
```

```{exercise}
For the regressions `y ~ x2` and `y ~ x3`, do the following:

- Check that $\boldsymbol{\beta}$ is indeed maximizing the likelihood as compared with Figure \@ref(fig:maximumlikelihood).
- Plot the fitted logistic curve and compare it with the one in Figure \@ref(fig:maximumlikelihood).

```

## Kernel regression estimation {#reg-kre}

### Nadaraya-Watson estimator {#reg-nw}

Our objective is to estimate the regression function $m$ nonparametrically. Due to its definition, we can rewrite it as follows:
\begin{align*}
m(x)=&\,\mathbb{E}[Y\vert X=x]\nonumber\\
=&\,\int y f_{Y\vert X=x}(y)\mathrm{d}y\nonumber\\
=&\,\frac{\int y f(x,y)\mathrm{d}y}{f_X(x)}.
\end{align*}
This expression shows an interesting point: the regression function can be computed from the joint density $f$ and the marginal $f_X$. Therefore, given a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$, an estimate of $m$ follows by replacing the previous densities by their kdes. To that aim, recall that in Exercise \@ref(exr:multkde) we defined a multivariate extension of the kde based on product kernels. For the two dimensional case, the kde with equal bandwidths $\mathbf{h}=(h,h)$ is
\begin{align}
\hat f(x,y;h)=\frac{1}{n}\sum_{i=1}^nK_{h}(x_1-X_{i})K_{h}(y-Y_{i}).(\#eq:kdem)
\end{align}
Using \@ref(eq:kdem),
\begin{align*}
m(x)\approx&\,\frac{\int y \hat f(x,y;h)\mathrm{d}y}{\hat f_X(x;h)}\\
=&\,\frac{\int y \hat f(x,y;h)\mathrm{d}y}{\hat f_X(x;h)}\\
=&\,\frac{\int y \frac{1}{n}\sum_{i=1}^nK_h(x-X_i)K_h(y-Y_i)\mathrm{d}y}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}\\
=&\,\frac{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)\int y K_h(y-Y_i)\mathrm{d}y}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}\\
=&\,\frac{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)Y_i}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}.
\end{align*}
This yields the so-called **Nadaraya-Watson**^[Termed due to the coetaneous proposals by @Nadaraya1964 and @Watson1964.] estimate:
\begin{align}
\hat m(x;0,h):=\frac{1}{n}\sum_{i=1}^n\frac{K_h(x-X_i)}{\frac{1}{n}\sum_{i=1}^nK_h(x-X_i)}Y_i=\sum_{i=1}^nW^0_{i}(x)Y_i, (\#eq:nw)
\end{align}
where $W^0_{i}(x):=\frac{K_h(x-X_i)}{\sum_{i=1}^nK_h(x-X_i)}$. This estimate can be seen as a weighted average of $Y_1,\ldots,Y_n$ by means of the set of weights $\{W_{n,i}(x)\}_{i=1}^n$ (check that they add to one). The set of weights depends on the evaluation point $x$. That means that the Nadaraya-Watson estimator is a **local mean of $Y_1,\ldots,Y_n$ around $X=x$** (see Figure \@ref(fig:kreg)).

Let's implement the Nadaraya-Watson estimate to get a feeling of how it works in practice.

```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# Nadaraya-Watson
mNW <- function(x, X, Y, h, K = dnorm) {

  # Arguments
  # x: evaluation points
  # X: vector (size n) with the predictors
  # Y: vector (size n) with the response variable
  # h: bandwidth
  # K: kernel

  # Matrix of size n x length(x)
  Kx <- sapply(X, function(Xi) K((x - Xi) / h) / h)

  # Weights
  W <- Kx / rowSums(Kx) # Column recycling!

  # Means at x ("drop" to drop the matrix attributes)
  drop(W %*% Y)

}

# Generate some data to test the implementation
set.seed(12345)
n <- 100
eps <- rnorm(n, sd = 2)
m <- function(x) x^2 * cos(x)
X <- rnorm(n, sd = 2)
Y <- m(X) + eps
xGrid <- seq(-10, 10, l = 500)

# Bandwidth
h <- 0.5

# Plot data
plot(X, Y)
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(xGrid, mNW(x = xGrid, X = X, Y = Y, h = h), col = 2)
legend("topright", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)
```

```{exercise}
Implement your own version of the Nadaraya-Watson estimator in `R` and compare it with `mNW`. You may focus only on the normal kernel and reduce the accuracy of the final computation up to `1e-7` to achieve better efficiency. Are you able to improve the speed of `mNW`? Use `system.time` or the `microbenchmark` package to measure the running times for a sample size of $n=10000$.

**Winner-takes-all bonus**: the significative fastest version of the Nadaraya-Watson estimator (under the above conditions) will get a bonus of 0.5 points.
```

The code below illustrates the effect of varying $h$ for the Nadaraya-Watson estimator using `manipulate`.

```{r, echo = TRUE, eval = FALSE, collapse = TRUE, cache = TRUE}
# Simple plot of N-W for varying h's
manipulate({

  # Plot data
  plot(X, Y)
  rug(X, side = 1); rug(Y, side = 2)
  lines(xGrid, m(xGrid), col = 1)
  lines(xGrid, mNW(x = xGrid, X = X, Y = Y, h = h), col = 2)
  legend("topright", legend = c("True regression", "Nadaraya-Watson"),
         lwd = 2, col = 1:2)

}, h = slider(min = 0.01, max = 2, initial = 0.5, step = 0.01))

```

### Local polynomial regression {#reg-locpoly}

Nadaraya-Watson can be seen as a particular case of a *local polynomial fit*, specifically, the one corresponding to a *local constant fit*. The motivation for the local polynomial fit comes from attempting to the minimize the RSS
\begin{align}
\sum_{i=1}^n(Y_i-m(X_i))^2.(\#eq:mrss)
\end{align}
This is not achievable directly, since no knowledge on $m$ is available. However, by a $p$-th order Taylor expression it is possible to obtain that, for $x$ close to $X_i$,
\begin{align}
m(X_i)\approx&\, m(x)+m'(x)(X_i-x)+\frac{m''(x)}{2}(X_i-x)^2\nonumber\\
&+\cdots+\frac{m^{(p)}(x)}{p!}(X_i-x)^p.(\#eq:mtay)
\end{align}
Replacing \@ref(eq:mtay) in \@ref(eq:mrss), we have that
\begin{align}
\sum_{i=1}^n\left(Y_i-\sum_{j=0}^p\frac{m^{(j)}(x)}{j!}(X_i-x)^j\right)^2.(\#eq:mder)
\end{align}
This expression is still not workable: it depends on $m^{(j)}(x)$, $j=0,\ldots,p$, which of course are unknown! The *great idea* is to set $\beta_j:=\frac{m^{(j)}(x)}{j!}$ and turn \@ref(eq:mder) into a linear regression problem where the unknown parameters are $\boldsymbol{\beta}=(\beta_0,\beta_1,\ldots,\beta_p)'$:
\begin{align}
\sum_{i=1}^n\left(Y_i-\sum_{j=0}^p\beta_j(X_i-x)^j\right)^2.(\#eq:minbe)
\end{align}
While doing so, an estimate of $\boldsymbol{\beta}$ automatically will yield estimates for $m^{(j)}(x)$, $j=0,\ldots,p$, and we know how to obtain $\hat{\boldsymbol{\beta}}$ by minimizing \@ref(eq:minbe). The final touch is to make the contributions of $X_i$ dependent on the distance to $x$ by weighting with kernels:
\begin{align}
\hat{\boldsymbol{\beta}}:=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}\sum_{i=1}^n\left(Y_i-\sum_{j=0}^p\beta_j(X_i-x)^j\right)^2K_h(x-X_i).(\#eq:hatb)
\end{align}
Denoting
$$
\mathbf{X}:=\begin{pmatrix}
1 & X_1-x & \cdots & (X_1-x)^p\\
\vdots & \vdots & \ddots & \vdots\\
1 & X_n-x & \cdots & (X_n-x)^p\\
\end{pmatrix}_{n\times(p+1)}
$$
and
$$
\mathbf{W}:=\mathrm{diag}(K_h(X_1-x),\ldots, K_h(X_n-x)),\quad
\mathbf{Y}:=\begin{pmatrix}
Y_1\\
\vdots\\
Y_n
\end{pmatrix}_{n\times 1},
$$
we can re-express \@ref(eq:hatb) into a *weighted least squares problem* whose exact solution is
\begin{align}
\hat{\boldsymbol{\beta}}&=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}} (\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})'\mathbf{W}(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})\\
&=(\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{Y}.(\#eq:betaw)
\end{align}

```{exercise}
Using the equalities given in Exercise \@ref(exr:ls), prove \@ref(eq:betaw).
```

The estimate for $m(x)$ is then computed as
\begin{align*}
\hat m(x;p,h):&=\hat\beta_0\\
&=\mathbf{e}_1'(\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{Y}\\
&=\sum_{i=1}^nW^p_{i}(x)Y_i,
\end{align*}
where $W^p_{i}(x):=\mathbf{e}_1'(\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{e}_i$ and $\mathbf{e}_i$ is the $i$-th canonical vector. Just as the Nadaraya-Watson, the local polynomial estimator is a *linear combination of the responses*. Two cases deserve special attention:

- $p=0$ is the *local constant estimator* or the Nadaraya-Watson estimator (Exercise \@ref(exr:nw)). In this situation, the estimator has explicit weights, as we saw before:
$$
W_i^0(x)=\frac{K_h(x-X_i)}{\sum_{j=1}^nK_h(x-X_j)}.
$$
- $p=1$ is the *local linear estimator*, which has weights equal to (Exercise \@ref(exr:loc)):
\begin{align}
W_i^1(x)=\frac{\hat s_2(x;h)-\hat s_1(x;h)(X_i-x)}{\hat s_2(x;h)\hat s_0(x;h)-\hat s_1(x;h)^2}K_h(x-X_i),(\#eq:we)
\end{align}
where $\hat s_r(x;h):=\frac{1}{n}\sum_{i=1}^n(X_i-x)^rK_h(x-X_i)$.

Figure \@ref(fig:kreg) illustrates the construction of the local polynomial estimator (up to cubic degree) and shows how $\hat\beta_0=\hat m(x;p,h)$, the intercept of the local fit, estimates $m$ at $x$.

(ref:kregtitle) Construction of the local polynomial estimator. The animation shows how local polynomial fits in a neighborhood of $x$ are combined to provide an estimate of the regression function, which depends on the polynomial degree, bandwidth, and kernel (gray density at the bottom). The data points are shaded according to their weights for the local fit at $x$. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kreg/).

```{r, kreg, echo = FALSE, fig.cap = '(ref:kregtitle)', screenshot.alt = "images/screenshots/kreg.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '100%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/kreg/', height = '1000px')
```

`KernSmooth`'s `locpoly` implements the local polynomial estimator. Below are some examples of its usage.

```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# Generate some data to test the implementation
set.seed(123456)
n <- 100
eps <- rnorm(n, sd = 2)
m <- function(x) x^3 * sin(x)
X <- rnorm(n, sd = 1.5)
Y <- m(X) + eps
xGrid <- seq(-10, 10, l = 500)

# Bandwidth
h <- 0.25
lp0 <- locpoly(x = X, y = Y, bandwidth = h, degree = 0, range.x = c(-10, 10),
               gridsize = 500)
lp1 <- locpoly(x = X, y = Y, bandwidth = h, degree = 1, range.x = c(-10, 10),
               gridsize = 500)

# Plot data
plot(X, Y)
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(lp0$x, lp0$y, col = 2)
lines(lp1$x, lp1$y, col = 3)
legend("bottom", legend = c("True regression", "Local constant",
                         "Local linear"),
       lwd = 2, col = 1:3)
```

```{r, echo = TRUE, eval = FALSE, collapse = TRUE, cache = TRUE}
# Simple plot of local polynomials for varying h's
manipulate({

  # Plot data
  lpp <- locpoly(x = X, y = Y, bandwidth = h, degree = p, range.x = c(-10, 10),
                 gridsize = 500)
  plot(X, Y)
  rug(X, side = 1); rug(Y, side = 2)
  lines(xGrid, m(xGrid), col = 1)
  lines(lpp$x, lpp$y, col = p + 2)
  legend("bottom", legend = c("True regression", "Local polynomial fit"),
         lwd = 2, col = c(1, p + 2))

}, h = slider(min = 0.01, max = 2, initial = 0.5, step = 0.01),
p = slider(min = 0, max = 4, initial = 0, step = 1))

```

## Asymptotic properties {#reg-asymp}

The purpose of this section is to provide some key asymptotic results for the bias, variance, and asymptotic normality of the local linear and local constant estimators. These provide useful insights on the effect of $p$, $m$, $f$, and $\sigma^2$ in the performance of the estimators. Proofs and detailed analysis are skipped; we refer the interested reader to @Ruppert1994, Section 5.3 of @Wand1995, and Section 3.2 of @Fan1996.

Along this section we will make the following assumptions:

- **A1**. $m$ is twice continuously differentiable.
- **A2**. $\sigma^2$ is continuous and positive.
- **A3**. $f$ is continuously differentiable and positive.
- **A4**. The kernel $K$ is a symmetric and bounded pdf with finite second moment and is square integrable.
- **A5**. $h=h_n$ is a deterministic sequence of bandwidths such that, when $n\to\infty$, $h\to0$ and $nh\to\infty$.

The bias and variance are expanded in their *conditional* versions on the predictor's sample $X_1,\ldots,X_n$. The reason of analyzing the conditional instead of the *unconditional* versions is avoiding technical difficulties that integration with respect to the predictor's density may pose.

```{theorem, label = "biasvarloc"}
Under **A1**--**A5**, the conditional bias and variance of the local constant ($p=0$) and local linear ($p=1$) estimators are
\begin{align}
\mathrm{Bias}[\hat m(x;p,h)\vert X_1,\ldots,X_n]&=B_p(x)h^2+o_\mathbb{P}(h^2),(\#eq:mbias)\\
\mathbb{V}\mathrm{ar}[\hat m(x;p,h)\vert X_1,\ldots,X_n]&=\frac{R(K)}{nhf(x)}\sigma^2(x)+o_\mathbb{P}((nh)^{-1}),(\#eq:mvar)
\end{align}
where
$$
B_p(x)=\frac{\mu_2(K)}{2}\left\{m''(x)+2\frac{m'(x)f'(x)}{f(x)}1_{\{p=0\}}\right\}.
$$
```

The bias and variance expressions \@ref(eq:mbias) and \@ref(eq:mvar) yield interesting insights:

- The bias decreases with $h$ *quadratically* for $p=0,1$. The bias at $x$ is directly proportional to $m''(x)$ if $p=1$ and affected by $m''(x)$ if $p=0$. This has the same interpretation as in the density setting:

    - The bias is negative in concave regions, *i.e.* $\{x\in\mathbb{R}:m(x)''<0\}$. These regions correspond to *peaks and modes of $m$*
    - Conversely, the bias is positive in convex regions, *i.e.* $\{x\in\mathbb{R}:m(x)''>0\}$. These regions correspond to *valleys of $m$*.
    - **The wilder the curvature $m''$, the harder to estimate $m$**.

- The bias for $p=0$ at $x$ is affected by $m'(x)$, $f'(x)$, and $f(x)$. Precisely, **the lower the density $f(x)$, the larger the bias**. And **the faster $m$ and $f$ change at $x$, the larger the bias**. Thus the bias of the local constant estimator is much more sensible to $m(x)$ and $f(x)$ than the local linear (which is only sensible to $m''(x)$). Particularly, the fact that it depends on $f'(x)$ and $f(x)$ is referred as the *design bias* since it depends merely on the predictor's distribution.

- The variance depends directly on $\frac{\sigma^2(x)}{f(x)}$ for $p=0,1$. As a consequence, **the lower the density and larger the conditional variance, the more variable is $\hat m(\cdot;p,h)$.** The variance decreases at a factor of $(nh)^{-1}$ due to the effective sample size.

An extended version of Theorem \@ref(thm:biasvarloc), given in Theorem 3.1 of @Fan1996, shows that **odd order polynomial fits are preferable to even order polynomial fits**. The reason is that odd orders introduce an extra coefficient for the polynomial fit that allows to reduce the bias, while at the same time they keep the variance unchanged. In summary, the conclusions of the above analysis of $p=0$ vs. $p=1$, namely that $p=1$ has smaller bias than $p=0$ (but of the same order) and the same variance as $p=0$, extend to the case $p=2\nu$ vs. $p=2\nu+1$, $\nu\in\mathbb{N}$. This allows to claim that local polynomial fitting *is an odd world* (@Fan1996).

Finally, we have the asymptotic pointwise normality of the estimator.

```{theorem}
Assume that $\mathbb{E}[(Y-m(x))^{2+\delta}\vert X=x]<\infty$ for some $\delta>0$. Then, under **A1**--**A5**,
\begin{align}
&\sqrt{nh}(\hat m(x;p,h)-\mathbb{E}[\hat m(x;p,h)])\stackrel{d}{\longrightarrow}\mathcal{N}\left(0,\frac{R(K)\sigma^2(x)}{f(x)}\right),(\#eq:mnorm1)\\
&\sqrt{nh}\left(\hat m(x;p,h)-m(x)-B_p(x)h^2\right)\stackrel{d}{\longrightarrow}\mathcal{N}\left(0,\frac{R(K)\sigma^2(x)}{f(x)}\right).(\#eq:mnorm2)
\end{align}

```

## Bandwidth selection {#reg-bwd}

Bandwidth selection is, as in kernel density estimation, of key practical importance. Several bandwidth selectors, in the spirit of the plug-in and cross-validation ideas discussed in Section \@ref(dens-bwd) have been proposed. There are, for example, a rule-of-thumb analogue (see Section 4.2 in @Fan1996) and a direct plug-in analogue (see Section 5.8 in @Wand1995). For simplicity, we will focus only on the **cross-validation** bandwidth selector.

Following an analogy with the fit of the linear model, we could look for the bandwidth $h$ such that it minimizes the RSS of the form
\begin{align}
\frac{1}{n}\sum_{i=1}^n(Y_i-\hat m(X_i;p,h))^2.(\#eq:badcv)
\end{align}
However, this is a bad idea. Attempting to minimize \@ref(eq:badcv) always leads to $h\approx 0$ that results in a useless interpolation of the data. Let's see an example.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Grid for representing (3.22)
hGrid <- seq(0.1, 1, l = 200)^2
error <- sapply(hGrid, function(h)
  mean((Y - mNW(x = X, X = X, Y = Y, h = h))^2))

# Error curve
plot(hGrid, error, type = "l")
rug(hGrid)
abline(v = hGrid[which.min(error)], col = 2)
```
The root of the problem is the comparison of $Y_i$ with $\hat m(X_i;p,h)$, since there is nothing that forbids $h\to0$ and as a consequence $\hat m(X_i;p,h)\to Y_i$. We can change this behavior if we compare $Y_i$ with $\hat m_{-i}(X_i;p,h)$, the **leave-one-out estimate** computed without the $i$-th datum $(X_i,Y_i)$:
\begin{align*}
\mathrm{CV}(h)&:=\frac{1}{n}\sum_{i=1}^n(Y_i-\hat m_{-i}(X_i;p,h))^2,\\
h_\mathrm{CV}&:=\arg\min_{h>0}\mathrm{CV}(h).
\end{align*}
The optimization of the above criterion might seem to be computationally expensive, since it is required to compute $n$ regressions for a *single* evaluation of the objective function.

```{proposition, label = "cv"}
The weights of the leave-one-out estimator $\hat m_{-i}(x;p,h)=\sum_{\substack{j=1\\j\neq i}}^nW_{-i,j}^p(x)Y_j$ can be obtained from $\hat m(x;p,h)=\sum_{i=1}^nW_{i}^p(x)Y_i$:
\begin{align*}
W_{-i,j}^p(x)=\frac{W^p_j(x)}{\sum_{\substack{k=1\\k\neq i}}^nW_k^p(x)}=\frac{W^p_j(x)}{1-W_i^p(x)}.
\end{align*}
This implies that
\begin{align*}
\mathrm{CV}(h)=\frac{1}{n}\sum_{i=1}^n\left(\frac{Y_i-\hat m(X_i;p,h)}{1-W_i^p(X_i)}\right)^2.
\end{align*}

```

Let's implement this simple bandwidth selector in `R`.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Generate some data to test the implementation
set.seed(12345)
n <- 100
eps <- rnorm(n, sd = 2)
m <- function(x) x^2 + sin(x)
X <- rnorm(n, sd = 1.5)
Y <- m(X) + eps
xGrid <- seq(-10, 10, l = 500)

# Objective function
cvNW <- function(X, Y, h, K = dnorm) {
  	sum(((Y - mNW(x = X, X = X, Y = Y, h = h, K = K)) /
  	       (1 - K(0) / colSums(K(outer(X, X, "-") / h))))^2)
}

# Find optimum CV bandwidth, with sensible grid
bw.cv.grid <- function(X, Y,
                       h.grid = diff(range(X)) * (seq(0.1, 1, l = 200))^2,
                       K = dnorm, plot.cv = FALSE) {
	obj <- sapply(h.grid, function(h) cvNW(X = X, Y = Y, h = h, K = K))
  h <- h.grid[which.min(obj)]
  if (plot.cv) {
    plot(h.grid, obj, type = "o")
    rug(h.grid)
    abline(v = h, col = 2, lwd = 2)
  }
  h
}

# Bandwidth
h <- bw.cv.grid(X = X, Y = Y, plot.cv = TRUE)
h

# Plot result
plot(X, Y)
rug(X, side = 1); rug(Y, side = 2)
lines(xGrid, m(xGrid), col = 1)
lines(xGrid, mNW(x = xGrid, X = X, Y = Y, h = h), col = 2)
legend("topright", legend = c("True regression", "Nadaraya-Watson"),
       lwd = 2, col = 1:2)
```

## Local likelihood {#reg-loclik}

We explore in this section an extension of the local polynomial estimator. This extension aims to estimate the regression function by relying in the likelihood, rather than the least squares. Thus, the idea behind the local likelihood is to **fit, locally, parametric models by maximum likelihood**.

We begin by seeing that local likelihood using the the linear model is equivalent to local polynomial modelling. Theorem \@ref(thm:lik) showed that, under the assumptions given in Section \@ref(reg-lin), the maximum likelihood estimate of $\boldsymbol{\beta}$ in the linear model
\begin{align}
Y|(X_1,\ldots,X_p)\sim\mathcal{N}(\beta_0+\beta_1X_1+\ldots+\beta_pX_p,\sigma^2)(\#eq:linmod)
\end{align}
was equivalent to the least squares estimate, $\hat{\boldsymbol{\beta}}_\mathrm{ML}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}\mathbf{Y}$. The reason was the form of the conditional (on $X_1,\ldots,X_p$) likelihood:
\begin{align*}
\ell(\boldsymbol{\beta})=&\,-\frac{n}{2}\log(2\pi\sigma^2)\nonumber\\
&-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_{i1}-\ldots-\beta_pX_{ip})^2.
\end{align*}
If there is a single predictor $X$, polynomial fitting of order $p$ of the conditional meal can be achieved by the well-known trick of identifying the $j$-th predictor $X_j$ in \@ref(eq:linmod) by $X^j$. This results in
\begin{align}
Y|X\sim\mathcal{N}(\beta_0+\beta_1X+\ldots+\beta_pX^p,\sigma^2).(\#eq:linmodp)
\end{align}
Therefore, maximizing with respect to $\boldsymbol{\beta}$ the *weighted log-likelihood of the linear model around $x$* of \@ref(eq:linmodp),
\begin{align*}
\ell_{x,h}(\boldsymbol{\beta})=&\,-\frac{n}{2}\log(2\pi\sigma^2)\\
&-\frac{1}{2\sigma^2}\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i-\ldots-\beta_pX_i^p)^2K_h(x-X_i),
\end{align*}
provides $\hat\beta_0=\hat m(x;p,h)$, the local polynomial estimator, as it was obtained in \@ref(eq:hatb).

The same idea can be applied for other parametric models. The family of *generalized linear models*^[The logistic model is a generalized linear model, as seen in Section \@ref(reg-log)], which presents an extension of the linear model to different kinds of response variables, provides a particularly convenient parametric framework. Generalized linear models are constructed by mapping the support of $Y$ to $\mathbb{R}$ by a link function $g$, and then modeling the *transformed* expectation by a linear model. Thus, a generalized linear model for the predictors $X_1,\ldots,X_p$, assumes
\begin{align*}
g\left(\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]\right)&=\beta_0+\beta_1x_1+\ldots+\beta_px_p
\end{align*}
or, equivalently,
\begin{align*}
\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]&=g^{-1}\left(\beta_0+\beta_1x_1+\ldots+\beta_px_p\right)
\end{align*}
together with a distribution assumption for $Y|(X_1,\ldots,X_p)$. The following table lists some useful transformations and distributions that are adequate to model responses in different supports. Recall that the linear and logistic models of Sections \@ref(reg-lin) and \@ref(reg-log) are obtained from the first and second rows, respectively.

| Support of $Y$ | Distribution | Link $g(\mu)$ | $g^{-1}(\eta)$ | $Y\vert(X_1=x_1,\ldots,X_p=x_p)$ |
|:--------------:|:------------:|:-----------:|:-----------:|:-----------------------------:|
| $\mathbb{R}$ | $\mathcal{N}(\mu,\sigma^2)$ | $\mu$ | $\eta$ | $\mathcal{N}(\beta_0+\beta_1x_1+\ldots+\beta_px_p,\sigma^2)$|
| $(0,\infty)$ | $\mathrm{Exp}(\lambda)$ | $\mu^{-1}$ | $\eta^{-1}$ | $\mathrm{Exp}(\beta_0+\beta_1x_1+\ldots+\beta_px_p)$ |
| $0,1$ | $\mathrm{B}(1,p)$ | $\mathrm{logit}(\mu)$ | $\mathrm{logistic}(\eta)$ | $\mathrm{B}\left(1,\mathrm{logistic}(\beta_0+\beta_1x_1+\ldots+\beta_px_p)\right)$ |
| $0,1,2,\ldots$ | $\mathrm{Pois}(\lambda)$ | $\log(\mu)$ | $e^\eta$ | $\mathrm{Pois}(e^{\beta_0+\beta_1x_1+\ldots+\beta_px_p})$ |

All the distributions of the table above are members of the exponential family of distributions, which is the family of distributions with pdf expressable as
\[
f(y;\theta,\phi)=\exp\left\{\frac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi)\right\},(\#eq:expd)
\]
where $a(\cdot)$, $b(\cdot)$, and $c(\cdot,\cdot)$ are specific functions, $\phi$ is the *scale parameter* and $\theta$ is the *canonial parameter*. If the *canonical link function* $g$ (the ones in the table above are all) is employed, then $\theta=g(\mu)$ and s a consequence
\[
\theta(x_1,\ldots,x_p):=g(\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]).
\]
Recall that, again, if there is only one predictor, identifying the $j$-th predictor $X_j$ by $X^j$ in the above expressions allows to consider $p$-th order polynomial fits for $g\left(\mathbb{E}[Y|X_1=x_1,\ldots,X_p=x_p]\right)$.

(ref:locliktitle) Construction of the local likelihood estimator. The animation shows how local likelihood fits in a neighborhood of $x$ are combined to provide an estimate of the regression function for binary response, which depends on the polynomial degree, bandwidth, and kernel (gray density at the bottom). The data points are shaded according to their weights for the local fit at $x$. Application also available [here](https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/loclik/).

```{r, loclik, echo = FALSE, fig.cap = '(ref:locliktitle)', screenshot.alt = "images/screenshots/loclik.png", dev = 'png', cache = TRUE, fig.pos = 'h!', out.width = '100%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/loclik/', height = '900px')
```

We illustrate the local likelihood principle for the logistic regression, the simplest non-linear model. In this case, $(X_1,Y_1),\ldots,(X_n,Y_n)$ with
$$
Y_i|X_i\sim \mathrm{Ber}(p(X_i)),\quad i=1,\ldots,n.
$$
for $p(x)=\mathbb{P}[Y=1|X=x]=\mathbb{E}[Y|X=x]$. The link function is
$g(x)=\mathrm{logit}(x)=\log\left(\frac{x}{1-x}\right)$ and $\theta(x)=\mathrm{logit}(p(x))$. Assuming that $\theta(x)=\beta_0+\beta_1x+\ldots+\beta_px^p$^[If $p=1$, then we have the usual logistic model.], we have that the log-likelihood of $\boldsymbol{\beta}$ is
\begin{align*}
\ell(\boldsymbol{\beta})
=&\,\sum_{i=1}^n\left\{Y_i\log(\mathrm{logistic}(\theta(X_i)))+(1-Y_i)\log(1-\mathrm{logistic}(\theta(X_i)))\right\}\nonumber\\
=&\,\sum_{i=1}^n\ell(Y_i,\theta(X_i)),
\end{align*}
where we consider the *log-likelihood addend*
\[
\ell(y,\theta)=y\theta-\log(1+e^\theta)
\]
and make explicit the dependence on $\theta(x)$ for clarity in the next developments, and implicit the dependence on $\boldsymbol{\beta}$. The *local log-likelihood of $\boldsymbol{\beta}$ around $x$* is then
\begin{align}
\ell_{x,h}(\boldsymbol{\beta})
=\sum_{i=1}^n\ell(Y_i,\theta(X_i-x))K_h(x-X_i).(\#eq:logploclik)
\end{align}
Maximizing^[No analytical solution for the optimization problem, numerical optimization is needed.] the local log-likelihood \@ref(eq:logploclik) with respect to $\boldsymbol{\beta}$ provides
$$
\hat{\boldsymbol{\beta}}=\arg\max_{\boldsymbol{\beta}\in\mathbb{R}^{p+1}}\ell_{x,h}(\boldsymbol{\beta}).
$$
The local likelihood estimate of $\theta(x)$ is
\[
\hat\theta(x):=\hat\beta_0.
\]
Note that the dependence of $\hat\beta_0$ on $x$ and $h$ is omitted. From $\hat\theta(x)$, we can obtain the *local logistic regression* evaluated at $x$ as
\begin{align}
\hat m_{\ell}(x;h,p):=g^{-1}\left(\hat\theta(x)\right)=\mathrm{logistic}(\hat\beta_0).(\#eq:glmloc)
\end{align}

The code below shows three different ways of implementing the local logistic regression (of first degree) in `R`.

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Simulate some data
n <- 200
logistic <- function(x) 1 / (1 + exp(-x))
p <- function(x) logistic(1 - 3 * sin(x))
set.seed(123456)
X <- runif(n = n, -3, 3)
Y <- rbinom(n = n, size = 1, prob = p(X))

# Set bandwidth and evaluation grid
h <- 0.25
x <- seq(-3, 3, l = 501)

# Optimize the weighted log-likelihood through glm's built in procedure
suppressWarnings(
  fitGlm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    glm.fit(x = cbind(1, X - x), y = Y, weights = K,
            family = binomial())$coefficients[1]
  })
)

# Optimize the weighted log-likelihood explicitly
suppressWarnings(
  fitNlm <- sapply(x, function(x) {
    K <- dnorm(x = x, mean = X, sd = h)
    nlm(f = function(beta) {
      -sum(K * (Y * (beta[1] + beta[2] * (X - x)) -
                  log(1 + exp(beta[1] + beta[2] * (X - x)))))
      }, p = c(0, 0))$estimate[1]
  })
)

# Using locfit
# Bandwidth can not be controlled explicitly - only though nn in ?lp
library(locfit)
fitLocfit <- locfit(Y ~ lp(X, deg = 1, nn = 0.25), family = "binomial",
                    kern = "gauss")

# Compare fits
plot(x, p(x), ylim = c(0, 1.5), type = "l", lwd = 2)
lines(x, logistic(fitGlm), col = 2)
lines(x,logistic(fitNlm), col = 2, lty = 2)
plot(fitLocfit, add = TRUE, col = 4)
legend("topright", legend = c("p(x)", "glm", "nlm", "locfit"), lwd = 2,
       col = c(1, 2, 2, 4), lty = c(1, 2, 1, 1))
```

Bandwidth selection can be done by means of *likelihood cross-validation*. The objective is to maximize the local likelihood fit at $(X_i,Y_i)$ but removing the influence by the datum itself. That is, maximizing
\begin{align}
\mathrm{LCV}(h)=\sum_{i=1}^n\ell(Y_i,\hat\theta_{-i}(X_i)),(\#eq:cvloclik)
\end{align}
where $\hat\theta_{-i}(X_i)$ represents the local fit at $X_i$ without the $i$-th datum $(X_i,Y_i)$. Unfortunately, the nonlinearity of \@ref(eq:glmloc) forbids a simplifying result as Proposition \@ref(prp:cv). Thus, in principle, it is required to fit $n$ local likelihoods for sample size $n-1$ for obtaining a single evaluation of \@ref(eq:cvloclik).

There is, however, an approximation (see Sections 4.3.3 and 4.4.3 of @Loader1999) to \@ref(eq:cvloclik) that only requires a local likelihood fit for a single sample. We sketch its basis as follows, without aiming to go in full detail. The approximation considers the first and second derivatives of $\ell(y,\theta)$ with respect to $\theta$, $\dot{\ell}(y,\theta)$ and $\ddot{\ell}(y,\theta)$. In the case of the logistic model, these are:
\begin{align*}
\dot{\ell}(y,\theta)&=y-\mathrm{logistic}(\theta),\\
\ddot{\ell}(y,\theta)&=-\mathrm{logistic}(\theta)(1 - \mathrm{logistic}(\theta)).
\end{align*}
It can be seen that (Exercise 4.6 in @Loader1999)
\begin{align}
\hat\theta_{-i}(X_i)\approx\hat\theta(X_i)-\mathrm{infl}(X_i)\left(\dot{l}(Y_i,\hat\theta(X_i))\right)^2,(\#eq:thetai)
\end{align}
where $\hat\theta(X_i)$ represents the local fit at $X_i$ and the *influence function* is defined as (page 75 of @Loader1999)
\[
\mathrm{infl}(x):=\mathbf{e}_1'(\mathbf{X}_x'\mathbf{W}_x\mathbf{V}\mathbf{X}_x)^{-1}\mathbf{e}_1K(0)
\]
for the matrices
\begin{align*}
\mathbf{X}_x:=\begin{pmatrix}
1 & X_1-x & \cdots & (X_1-x)^p/p!\\
\vdots & \vdots & \ddots & \vdots\\
1 & X_n-x & \cdots & (X_n-x)^p/p!\\
\end{pmatrix}_{n\times(p+1)}
\end{align*}
and
\begin{align*}
\mathbf{W}_x&:=\mathrm{diag}(K_h(X_1-x),\ldots, K_h(X_n-x)),\\
\mathbf{V}&:=-\mathrm{diag}(\ddot{l}(Y_1,\theta(X_1)),\ldots, \ddot{l}(Y_n,\theta(X_n))).
\end{align*}
A one-term Taylor expansion of $\ell(Y_i,\hat\theta_{-i}(X_i))$ using \@ref(eq:thetai) gives
\[
\ell(Y_i,\hat\theta_{-i}(X_i))=\ell(Y_i,\hat\theta(X_i))-\mathrm{infl}(X_i)\left(\dot{\ell}(Y_i,\theta(X_i))\right)^2.
\]
Therefore:
\begin{align*}
\mathrm{LCV}(h)&=\sum_{i=1}^n\ell(Y_i,\hat\theta_{-i}(X_i))\\
&\approx\sum_{i=1}^n\ell(Y_i,\hat\theta(X_i))+\mathrm{infl}(X_i)\left(\dot{\ell}(Y_i,\hat\theta(X_i))\right)^2.
\end{align*}
Recall that $\theta(X_i)$ are unknown and hence they must be estimated.

We conclude by illustrating how to compute the LCV function and optimize it (keep in mind that much more efficient implementations are possible!).

```{r, echo = TRUE, eval = TRUE, collapse = TRUE, cache = TRUE}
# Exact LCV - recall that we *maximize* the LCV!
h <- seq(0.1, 2, by = 0.1)
suppressWarnings(
  LCV <- sapply(h, function(h) {
  sum(sapply(1:n, function(i) {
    K <- dnorm(x = X[i], mean = X[-i], sd = h)
    nlm(f = function(beta) {
      -sum(K * (Y[-i] * (beta[1] + beta[2] * (X[-i] - X[i])) -
                  log(1 + exp(beta[1] + beta[2] * (X[-i] - X[i])))))
      }, p = c(0, 0))$minimum
    }))
  })
)
plot(h, LCV, type = "o")
abline(v = h[which.max(LCV)], col = 2)
```

## Exercises {#reg-exercises}

This is the list of evaluable exercises for Chapter \@ref(reg). The number of stars represents an estimate of their difficulty: easy ($\star$), medium ($\star\star$), and hard ($\star\star\star$).

```{exercise, label = "nw"}
(theoretical, $\star$) Show that the local polynomial estimator yields the Nadaraya-Watson when $p=0$. Use \@ref(eq:betaw) to obtain \@ref(eq:nw).
```

```{exercise}
(theoretical, $\star\star$) Obtain the optimization problem for the local Poisson regression (for the first degree) and the local binomial regression (of first degree also).
```

<!-- 
Provide the corresponding approximate expression for the cross-validation given in \@ref(eq:approxcv)
-->

```{exercise}
(theoretical, $\star\star$) Show that the Nadaraya-Watson is unbiased (in conditional expectation with respect to $X_1,\ldots,X_n$) when the regression function is constant: $m(x)=c$, $c\in\mathbb{R}$. Show the same for the local linear estimator for a linear regression function $m(x)=a+bx$, $a,b\in\mathbb{R}$. *Hint*: use \@ref(eq:we).
```

```{exercise, label = "loc"}
(theoretical, $\star\star\star$) Obtain the weight expressions \@ref(eq:we) of the local linear estimator. *Hint:* use the matrix inversion formula for $2\times2$ matrices.
```

```{exercise}
(theoretical, $\star\star\star$) Prove the two implications of Proposition \@ref(prp:cv) for the Nadaraya-Watson estimator ($p=0$).
```

```{exercise}
(practical, $\star\star$, Example 4.6 in @Wasserman2006)
The dataset at <http://www.stat.cmu.edu/~larry/all-of-nonpar/=data/bpd.dat> (alternative [link](https://github.com/egarpor/nonpar-eafit/blob/master/datasets/bdp.txt?raw=true)) contains information about the presence of bronchopulmonary dysplasia (binary response) and the birth weight in grams (predictor) of 223 babies. Use the function `locfit` of the `locfit` library  with the argument `family = "binomial"` and plot its output. Explore and comment on the resulting estimates, providing insights about the data.
```

```{exercise}
(practical, $\star\star$)
The `ChickWeight` dataset in `R` contains 578 observations of `weight` and `Times` of chicks. Fit a local binomial or local Poisson regression of `weight` on `Times`. Use the function `locfit` of the `locfit` library  with the argument `family = "binomial"` or `family = "poisson"` and explore the bandwidth effect. Explore and comment on the resulting estimates. What is the estimated expected time of a chick that weights 200 grams? 
```

```{exercise}
(practical, $\star\star\star$) Implement your own version of the local linear estimator. The function must take a sample `X`, a sample `Y`, the points `x` at which the estimate should be obtained, the bandwidth `h` and the kernel `K`. Test its correct behavior by estimating an artificial dataset that follows a linear model.
```

```{exercise}
(practical, $\star\star\star$) Implement your own version of the local likelihood estimator of first degree for exponential response. The function must take a sample `X`, a sample `Y`, the points `x` at which the estimate should be obtained, the bandwidth `h` and the kernel `K`. Test its correct behavior by estimating an artificial dataset that follows a generalized linear model with exponential response, this is,
\[
Y|X=x \sim \mathrm{Exp}(\lambda(x)),\quad \lambda(x)=e^{\beta_0+\beta_1x},
\]
using a cross-validated bandwidth. *Hint*: use `optim` or `nlm` for optimizing a function in `R`.
```

<!--
```{exercise}
(practical, $\star\star\star$) Compare the exact \@ref(eq:cvloclik) and approximate \@ref(eq:approxcv) cross-validation functions for the local logistic regression. To that aim, plot several realizations both curves and their minima, for samples simulated from $X\sim\mathcal{U}(-1,1)$ and $Y|X=x\sim\mathrm{Ber}(\mathrm{logistic}(1 + x))$. Use $\theta(X_i)\approx\hat\theta(X_i)$. Use sample sizes $n=50, 100, 500$. Comment on the results.
```
-->
